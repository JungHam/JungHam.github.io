# CS231n 7강

저번시간 복습 

internal covariant : 데이터 간의 분포가 달라지면 학습 결과가 달라질 수 있다. 

Batch Normalization의 위치 : non-linearble 연산이 필요한 경우

그래서 normalization하는 단계는 2단계(normalization -> network 학습 변수 적용)

모멘텀 기법 + gradient squared term

```python
vx = 0
while True :
    dx = compute_gradient(x)
    vx = rho * vx + dx
    x += learning_rate * vx
```

여기서 rho값이 velocity의 영향력을 조절하는데, 보통 0.9, 0.99와 같이 높은 값을 준다. 그래서 gradient vector가 바로 반영되는게 아니라 velocity의 영향을 받아서 나아가게 된다. 

### Adam

bias correction에 대해서 좀 더 서술하자.

momentum + ada = adam

```python
first_moment = 0
second_moment = 0
for i in range(1, num_iterations) :
	dx = compute_gradient(x)
    # for momentum
    first_moment = beta1 * first_moment + (1-beta1) * dx
    # for RMSProp
    second_moment = beta2 * second_moment + (1-beta2) * dx * dx
    # bias correction start
    first_unbias = first_moment / (1-beta1**t)
    second_unbias = second_moment / (1-beta2**t)
    # bias correction end
    # AdaGrad / RMSProp
    x -= learning_rate * first_unbias / (np.sqrt(second_unbias) + 1e-7)
```

여기서 beta2 = 0.999, learning_rate = 1e-3 또는 5e-4

첫째 둘째 모멘트를 계산하고 난 다음에 그 둘에서 unbias 값을 계산한 후, 이를 다음 스텝에서 적용한다. (베타, 알파)

만약 unbias를 계산하지 않으면 다음의 문제가 생긴다. first_moment와 second_moment가 0에서 시작하는데, second_moment는 첫번째 스텝이 지나가도 0에 수렴하게 된다. 이를 보정하기 위해 Bias correction을 한다. 

처음에는 learning rate를 높여서 학습하다가 그 이후에는 꾸준히 낮추면서 학습한다. (learning rate decay)

이 기법은 adam보다 sgd에서 더 많이 쓰이고, 부차적인 하이퍼파라미터가 된다. 정확도를 지켜보다가 필요하다 싶을 때 learning rate를 변경하면서. 

First-order optimization : 그 시점에서의 접선의 기울기를 구하는 방법 ->  second-order optimization이 나온듯.

1차 미분은 기울기, 2차 미분은 기울기의 양상을 구할수 있다! 

Second-order optimization : 2차 미분의 양상을 사용한다. First-order를 사용하는 것보다 좋긴 한데, 사용할 수 있는 곳이 정해져 있다. Newton step. Hessian matrix. 여기서는 learning rate가 없다. 하이퍼파라미터도 없다. Hessian은 N^3를 사용하기 때문에, 딥러닝에서 활용할 수가 없다(무수한 차원을 생각해보라)

그래서 Quasi-Newton에서는 N^2로 줄지만 이래도 딥러닝에 활용하기에는 너무 크다. 만약 full-batch를 시도할거면, L-BFGS를 사용해봐라. (하지만 Adam을 쓰세요...)

#### Beyond Training Error

regularization에 대해서 다뤄볼 에정. 

train과 test(validation)의 학습 결과를 줄이기 위해서 어떤 것을 해야 하나? : 독립적인 여러 모델 쓰고 앙상블 기법을 써서 결과의 평균을 사용하기. 근데 2프로 정도 증가한다고... (ex : imagenet) 

좀 더 나아가면 모델들을 저장하고(snapshot) 앙상블로 사용해볼 수 있다. 학습 중간중간에 앙상블의 결과를 보고 이를 학습에 반영한다. 그래서 learning rate를 높였다 낮췄다 퐁당퐁당하면서 지나간다. 

만약 평균을 낼 때 모델 간의 결과의 gap을 신경쓰지 않는다면 validation에 overfitting하면 되지 않을까 라는 단순한 원리로 접근할 수도 있다. poiyak averaging이라는 기법이 있다. 

단일 모델의 성능을 향상시키려면 어떻게 해야할까? 

#### Regularization : Add term to loss 

Training할 때 random noise를 더하고, testing때는 noise를 평균화.

L1, L2, Elastic net 

Fully connected layer -> 어떤 뉴런들을 렌덤하게 반영하지 않는다. 근데 너무 많은 특징을 반영하게 되면 랜덤하게 누락된 뉴런이 특징을 파악할 때 관여하는 경우 불리할 수 있다. 

Dropout : Random mask를 쓰는 방법 (<- 후에 복습하기!) 

그렇다면 이를 test time에서 어떻게 dropout을 사용할까? Dropout한 만큼 test time에서 값을 증폭해야 한다. 하지만 inverted dropout이라는 것도 있는데 이건 train할때 떨군 확률 p를 나눠주고, test time에서는 이를 그냥 둔다. 

average out ---> 이거 나중에 복습하기!

dropout이 적용되지 않은 노드에서만 back propagation이 일어난다. 결국 Reularization은 네트워크에 무작위성을 추가해서 train 데이터에 overfitting하는 문제를 막는다.  BN이랑 비슷한 효과를 얻을 수 있다. Dropout은 BN과 달리 자유롭게 조정할 수 있는 파라미터 p(drop 확률)가 존재한다. 

#### Data Augmentation 

이미지의 일부를 잘라서 변형시켜 이를 test에 사용할 수 있다. 그리고 이 외에도 다양한 방법이 있는데, 핵심은 label을 유지하면서 데이터에 변형을 가하는 방법이다. 

Fractional maxpooling : pooling할 지역이 random하게 지정된다. 

stochastic depth에 대해서 : 곁보기에는 바로 직전의 결과물이 아닌 몇 스텝 이전의 결과물을 그 다음에 반영하는 걸로 보이는데...

보통은 BN을 많이 쓰지만 overfitting이 해결되지 않는다면 Dropout과 같은 다양한 방법을 추가해볼 수 있다.  



transfer learning(꽤 유용한 방법) -> 해커톤 같은 경우에 사용가능하다. 

데이터가 얼마 없는 경우에 transfer learning을 사용한다. Fully connected layer쪽을 다시 학습시키는 방법을 사용한다. 비슷한 데이터셋의 경우에는 데이터셋 크기 작으면 선형 분류, 많으면 몇몇 레이어를 잘 튜닝시키고 다른 데이터들이 있는 데이터셋의 경우에는 다르게 대처한다. 

데이터가 1M 이하면 transfer learning을 추천한다. 



# CS231n-09

CNN output size calculator

you can use this formula `[(W−K+2P)/S]+1`.

- W is the input volume - in your case 128
- K is the Kernel size - in your case 5
- P is the padding - in your case 0 i believe
- S is the stride - which you have not provided.

파라미터의 수는 96 * 11 * 11 * 3

pooling layer에는 parameter가 없다. 왜냐하면 파라미터는 우리가 학습하는 가중치를 의미하는데, pooling의 연산은 학습하면서 적용할 가중치가 없기 때문이다. 

AlexNet의 경우 모델을 2개로 나누어서 학습했는데 이유는 GPU 메모리 용량때문에(...)

VGGNet에 대해서 설명 : 

Small filters, Deeper networks. 

왜 크기가 작은 필터를 사용했나? 크기가 작은 필터를 여러 레이어에 걸쳐 적용한 것은 크기가 큰 필터를 한번 적용한 것과 같다. ( ex : 3x3 필터 세 번과 7x7 필터 한번 )