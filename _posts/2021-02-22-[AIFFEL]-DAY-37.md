# Fundamental

## 딥러닝 레이어의 이해(1) : linear, convolution

- 레이어의 정의 : 하나의 물체가 여러 개의 논리적인 객체들로 구성되어 있는 경우, 각 개체를 하나의 레이어라 정의함

### Linear layer

- 별칭 : Fully Connected Layer, Feedforward Neural Network, Multilayer perceptrons, Dense Layer...
- Weight의 모든 요소를 Parameter라고 부른다. 

텐서플로우의 주요 코드 설명

```python
import tensorflow as tf

### 중간 생략
### (4,2) 차원인 boxes를 (4,3)으로 확장시키는 Linear Layer
boxes = tf.zeros((batch_size, 4, 2))
first_linear = tf.keras.layers.Dense(units=3, use_bias=False)
first_weight_shape =  first_linear.weights[0].shape

### 4차원의 입력값을 하나의 실수로 만드는 Linear Layer
### 2단계를 거쳐야 한다
### 먼저 (4,3) -> (4,1) 로 만들어 준다. 
second_linear = tf.keras.layers.Dense(units=1, use_bias=False)
second_out = second_linear(first_out)
second_out = tf.squeeze(second_out, axis=-1)
### 그리고 (4,1) 을 1차원 데이터로 바꿔주기 위해 Dense를 한번 더 거친다. 
third_linear = tf.keras.layers.Dense(units=1, use_bias=False)
third_out = third_linear(second_out)
third_out = tf.squeeze(third_out, axis=-1)

```

- Convolution Layer

- pooling layer : 핵심만 추리는 것

  Receptive Field : 뉴럴 네트워크의 출력부에서 충분한 정보를 얻기 위해 커버하는 입력 데이터의 인식 영역이 커서, 그 안에 찾고자 하는 객체의 특성이 충분히 포함되어 있어야 정확한 인식이 가능하게 된다. 

- Max pooling의 장점
  - translational invariance : 객체 위치에 대한 오버피팅 방지
  - non-linear 함수와 동일한 피처 추출 효과
  - receptive field 극대화 효과 : max pooling 없이 이런 효과를 보려면 convolutional layer를 수없이 쌓아야 한다. 
  - cf ) dilated convolution(팽창된 컨볼루션) : 이전의 필터는 가중치들이 모여있는 형태라면 이 필터를 사방으로 밀어서 팽창시킨다. 풍선에 점 찍어놓고 바람 부는 것을 생각해 보자. 

## CS231n-10

Recurrent Neural Networks(RNN)



vanila neural network : feed-forward neural network(linear layer)

연속적인 데이터에 대해 일어나는 학습의 경우

RNN은 가변길이의 데이터에 대응할 수 있다. 혹은 입/출력이 고정된 경우라 해도 '가변 과정'인 경우에 유리하다. 

 숫자 데이터에 대해서 어떻게 대응하는지 예시를 보여준다. 

RNN의 구조는 재귀적이다. 이전 스텝의 결과와 어떤 시점의 벡터를 함수의 입력으로 받는다. 여기서 쓰이는 함수와 함수에서 사용하는 가중치(W)는 변화하지 않는다. 

각 단계에서 같은 W를 쓴다. 

Many -to-Many의 경우는 각 단계의 출력이 output이고, local loss를 구해서 전체의 loss를 구한다. -> 각 단계에서 개별로 local Gradient들을 구해서 W에 업데이트한다.

Many-to-One의 경우는 마지막 히든 스테이지의 출력이 결과값이 된다. 

One-to-Many는 입력은 고정이지만 출력은 가변이다. 입력값이 초기 히든 스테이트를 초기화한다. 



그렇다면 Many-to-one과 one-to-many를 합치면? Sequence to Sequence

Encoder(many to one)와 Decoder(one to many)로 이루어진다. 입력을 하나의 백터로 요약하고, 하나의 벡터 입력에서 출력을 얻어낸다. '자연어'처리와 관련되어있는 모델이다. 

 input -> hidden -> output -> softmax(output에서 sample을 뽑기 위해 확률분포를 사용) -> sample -> 다음 인풋으로 sample 넣어주기(input과 sample은 둘 다 정보를 가진 vector. 예를 들면 one-hot-vector로)

근데 왜 확률분포를 사용하는가? 모델에서 다양성을 얻기 위해서. 항상 같은 접두사나 같은 이미지를 넣을 경우 argmax로 sample을 고르면 다양한 값이 나오지 않을 수 있다. 

'sampling' -> softmax로 확률분포를 취한다고 해도 sampling 설명이 어색해서, 'teacher forcing'을 도입해서 사용한다. 원래는 확률이 가장 높은 것을 취한다고 해도, '의도하는 바(자연어는 말하는 순서가 정해져 있으니까 순서라고 얘기할 수 있다.)를 강요'하는 것이다.  

Truncated Backpropagation through time

만약 시퀸스의 길이가 엄청나게 길어진다면 앞부분의 정보가 사라지는 단점이 있다. 그래서 긴 시퀸스를 쪼개서 덩어리로 보는 개념이 Truncated Backpropagation이다. 결국 배치로 잘라서 보는 것이다. 

RNN에는 hidden vector가 있고 이 vector를 계속 업데이트한다. RNN의 Cell은 보통 해석하기 힘들다. 해석할 수 있는 cell을 찾을 때 까지 가 보면, 그 모델이 학습하고자 하는 목표에 가까워지는 것을 볼 수 있다. 

