---
title:  "[AIFFEL] DAY 10"
excerpt: #

categories:
  - blog
tags:
  - [AI, AIFFEL, MODULAB]

toc: true
toc_sticky: true
 
date: 2021-01-11
last_modified_at: #
---


# 20210111



오전 : 머신러닝이란? 딥러닝이란? 에서 출발하는 질문들을 점차 심층적으로 내려가는 질문들의 사례를 보았다. 



## CS231n-03-02



### Optimization

계곡에 있다고 생각하고, 우리는 계곡의 가장 낮은 밑바닥을 찾아내는 것이 목적이다. 지금 우리가 있는 곳의 높이가 loss, 그리고 밑바닥을 찾아가는 여정이 optimization과정이 된다. 풍경은 파라미터 W를 의미한다. W는 loss에 따라서 달라지니까.    



loss를 줄이는 방법은 Random search 방법이 있다.  (정확도 15.5퍼센트 정도 나온다고...)   

두 번째 방법으로 경사의 기울기에 따라서 길을 찾아볼까?(NN, Linear regression)   

특정 지점에서 미분을 해서 기울기를 구해보자. 그렇다면 각 변수에 대한 편미분함수의 조합이 되겠지. 유닛벡터와 gradient의 조합이 해당 지점에서의 기울기를 알려줄 것이다.    

##### 수치적인 방법

현재 W->W+h(W의 각 dimension에 대해서 이를 반복한다.)->gradient인 dW 확인 => 이거 진짜 느리고 각 dimension에 대해서 함수 계산을 다시 계산해야 해서 시간 너무 걸린다. **다만 디버깅을 위해서 쓰일 수 있다.** 유닛 테스트 처럼. 이때는 파라미터를 줄여서 실시한다. (gradient check)   

정리하면 근접적이고, 느리고, 적기에는 편하다. 

##### 분석적인 방법 

시간을 축소하기 위해 걸리는 방식 : loss function을 적어놓고 미분함수를 구한 후 이의 극한을 구한다.    

정리하면 빠르고, 정확하다. 근데 실수하기 쉬움.

![Tuning the learning rate in Gradient Descent | Datumbox](https://blog.datumbox.com/wp-content/uploads/2013/10/gradient-descent.png)

##### Gradient descent 

W를 임의의 값으로 초기화한다. 그리고 gradient(접평면에 수직인 백터)가 -로 나오는 방향으로 weight를 조정해줄 것이다. 스텝 사이즈를 통해서 학습을 했을 때, 그 학습의 속도를 Learning Rate라고 한다. Step size는 하이퍼파라미터의 하나이다. 학습의 방향에 중요한 역할을 한다.   

효과적인 Gradient descent를 위해서 이전 Gradient descent 결과에 피드백을 주는 방법이 있다. 

Q. step size vs learning rate vs batch size 

step size : 보폭의 크기

learning rate : 보폭의 크기 내에서도 한번에 학습하는 정도...?

Gradient descent에 대해서 더 참고하려면 여기를~

https://seamless.tistory.com/38

![img](https://t1.daumcdn.net/cfile/tistory/993D383359D86C280D)

##### Stochastic Gradient Descent(SGD)

만약 N이 엄청나게 크면 어떡하지? 실제 gradient를 계산하려면 N개의 데이터를 돌면서 다 반영해야 할 것이다. 그래서 전체 데이터 셋의 gradient와 loss를 다 계산하기 보다는 Minibatch를 쓴다.(2의 승수를 주로 쓴다.) 그러니까 전체 집단 보다 데이터 셋에서 임의의 minibatch를 구해서 거기서 loss와 gradient를 구한다. 

cf) Optimizer 계보 참조해서 넣기

##### Image Features 학습하기

이전에는 여러 속성들을 벡터로 엮어서 Linear classification에 입력했다. Feature들을 transform해서 선형 분류가 되도록 바꾼다거나. 예를 들자면 컬러 히스토그램이 있다. 이미지에서 Hue값만 뽑아서 색깔 테이블에 매칭해 보는 것이다. 

Histogram Oriented Gradient : local gradient을 통해서 각 local별로 어떤 edge가 존재하는지 알아볼 수 있다. 그리고 해당 local에서 어떤 vector가 많이 등장하는지 양동이에 담는다. 예를 들어 자연어 처리에서 Bag of Words에서 문장의 여러 단어의 발생 빈도를 재서 양동이에 담는 경우를 떠올릴 수 있다. 이미지의 경우는 많은 이미지를 구해서, 이 이미지를 조각조각낸 후, 그 조각에서 
"virtual words"를 찾아낸다. 이를 모은 것을 "code book"이라고 할 때, 이 code book에 따라 이미지를 분류할 수 있을 것이다. 

10년 전만 해도 이미지 입력->BOW/HOG->특징 추출->분류기에 입력. 한번 이미지의 속성이 추출되면, 이는 변하지 않는다. 분류기만 학습이 될 뿐. 

딥러닝은 이제 이 속성까지 스스로 추출해 낸다는 것! 



## CS231-04-01

### Backpropagation and Neural Networks

어떻게 분석적인 방법을 통해 gradient descent를 실행하는가?

computational graph를 그려서 뉴럴 네트워크의 구조를 설명하더라. 

##### Backpropagation 

뒤에서부터 앞으로 반대방향으로 편미분을 한다. 

그렇다면 Backpropagation을 왜 하냐?(What is backpropagation really doing?) : 해당 네트워크가 제대로 학습하고 있는지를 알아보기 위해서 디버깅하는 과정이다. output으로 loss값을 구했다면, 이 다음 학습을 위해 weight를 새롭게 정비해야 한다. 따라서 역으로 거슬러 올라가면서 weight 값들을 변경하게 된다. 

local gradient (편미분을 통해서) 

해당 노드에서 local gradient를 구할려면 이전 노드 결과 x, y가 있다 치면 최종 loss function을 x,y로 편미분한 값들을 구한다. (chain rule을 통해 편미분 구함) output 값인 loss값이 상수이기 때문에 여기서 local gradient를 구할 수 있고, 이는 이전의 W에 -피드백을 하기 위해 쓰인다.(Gradient가 -여야 아래로 내려가는 방향의 피드백을 할 수 있기 때문에. 그래프를 생각해 보자.)

결론 : 정방향으로 갈 때는 모델에 데이터가 지나가면서 최종 학습 결과를 내놓는다. 역방향으로 갈 때는 최종 학습 결과와 loss값을 이용해서 각 레이어 사이의 weight값을 조정한다. 왔다 갔다, 핑퐁핑퐁. 



### 후기 

아침에 진짜 읽어볼 양이 많았다. 노드를 수행하기가 좀 버거웠고, 아침에 약한 타입임을 다시 한번 느꼈다. 이런. 오후 시간은 Backpropagation이 왜 이루어지는지 드디어 알아냈다! 학부시간에 주입식으로 들었던 시간들이 억울할 지경이다. 오늘도 이렇게 한발짝 나아간다. 













