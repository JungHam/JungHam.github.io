항상 gem과 jekyll이 문제다. 분명 그전날 세팅 다 했는데 다음날 충돌하는 모양을 보고 있노라면, 매우 허무해진다. 그래서 ruby부터 다시 깔았으므로, 다음에 충돌이 날 때는 어쩔 수 없다. PATH 확인하고, bundle이 의존하는 ruby를 에러메세지에서 주의깊게 살펴본 후 도저히 안되면 날려버리고 새로 세팅하라. 

# CS231n-06 

## Batch normalization

배치로 얻은 데이터가 나왔을 때, 각 배치의 평균과 분산을 통해 normalization 수행.

각 차원에서 평균과 분산을 구하고, 여기서 normalization을 수행한다. 보통 FC나 Convolutional layers 이후, 혹은 비선형 단계 전에 행한다. 

근데 activation function의 input이 반드시 가우시안 분포를 따라야 하는가? 우리의 목표는 activation function에 들어오는 input이 의미있는 activation function의 output값을 가지도록 처리하는 것이다. 

CNN에서는 activation map에서 데이터의 공간적인 관계를 유지하길 원하니까, 각 activation map간의 평균과 분산을 구해서 normalization 을 시도한다. 

`((layer input)-평균)/(분산) = 가우시안 분포` 

평균을 빼면 데이터가 0을 원점으로 위치하게 되고 분산으로 나누게 되면 데이터들이 0과 1사이에 분포하게 된다. 

## Babysitting the learning process

학습 과정이 어떻게 이루어지는지 보고 하이퍼파라미터를 어떻게 조정할 것인지에 대해서 배울 것. 

네트워크를 초기화한다. foward pass를 시켜보고 loss가 제대로 이루어지는지 확인한다. 그리고 데이터의 일부만 학습시켜 본다. Overfit과 Loss가 생길거다. 이 때는 regularization을 하지 않는다. 그리고 아주 조금의 regularization을 적용해본다. 이렇게 조금씩 값을 바꿀 경우 이 상황의 경우 결과 값들의 확률이 고르게 분포하고 있고, accuracy는 이 비슷비슷한 값 중에서 제일 큰 값을 취하기 때문에 accuracy가 갑자기 커질 수 있다. learning rate를 키우게 되면 cost가 어떻게 되나? 너무 키우면 cost가 발산해 버린다. 



## Hyperparameter Optimization 

처음에는 파라미터의 효율성을 알기 위해서 적은 횟수의 학습을 돌린다. 두 번째에서는 더 많이 반복해서 더 정제된 결과를 얻는다. cost가 이전의 3배 이상으로 발산하면 잘못된 파라미터를 가지고 학습하고 있는 것. validation accuracy를 넣어서 학습하는데, 파라미터 값은 log scale의 값으로 샘플링하자. (이유 후술)  

grid search는 어떤가? 하이퍼파라미터를 일정한 간격으로 증감시켜 파라미터를 넣어보는 것이다. 실제로는 random search가 더 잘 작동하지만. 

validation accuracy는 높아지지 않는다. 

# CS231n-07 



## Fancier optimization 

손실 함수가 가중치에 대한 landscape라고 생각하자. gradient descent를 줄여나가는 과정에서, 기존의 SGD의 경우 목표축이 아닌 축의 방향에 대해서는 지그재그로 값이 흔들리면서 느리게 업데이트 되는 것을 볼 수 있다. 그리고 SGD에서 발생하는 다른 문제점은 local minima와 saddle point이다. 둘 다에 걸려버린다. saddle point는 gradient가 0에 가깝기 때문이다.  

saddle point : 안장점. 변곡점이랑은 차원에 따라 같은 개념으로 취급하는 듯. 3차원으로 가면 2개의 변화지점이 겹치는 지점. 

### 부제 : 어떻게 learning rate와 step의 크기를 제어할 수 있을까? 

Momentum을 추가하면 이러한 흔들림을 해결할 수 있다. 파라미터 하나가 더 추가되는듯. Gradient와 velocity(gradient의 평균값)의 연산을 통해 실제 움직이는 과정을 정할 수 있다. Momentum, Nesterov Momentum. Nesterov Momentum에서는 Loss와 Gradient를 같은 점에서 계산하는 기존의 방식에서 살짝 변형을 주어 이전과 현재의 velocity간의 에러 보정이 들어가있다. Momentum은 단어 그대로 이전까지의 변화량을 바탕으로, 다음 스텝의 움직임에 이 변화량을 반영해 바로 gradient의 변화가 이루어지는게 아닌 이전 방향의 움직임에 영향을 받도록 하는 기법이다. 

근데 만약 minima가 길고 좁을때 그냥 지나치면 어떡해? > 흥미로운 부분이다. 사실 이런 특징을 가진 데이터는 바람직하진 않은데, 그래도 이걸 어떻게 해결할까에는 중요한 연구과제. 

AdaGrad는 훈련 도중의 gradient를 사용한다. 앞서 게산한 gradient를 제곱한 값을 사용한다. 그래서 기울기가 큰 경우에는 속도가 느려지고, 기울기가 작은 경우에는 속도가 빨라진다. 이렇게 되면 학습 횟수가 늘어난다. 손실함수가 convex한 경우에는 minimun에 근접하면서 서서히 속도를 줄이는 게 좋은 현상이 될 수 있지만, non-convex한 케이스에서는 제대로 작동하지 않을 수 있다. 

그래서 RMSProp에서 이러한 문제를 개선한다. AdaGrad는 learning rate의 문제로 잘 사용하지 않는다. 근데 왜 convex에서도 AdaGrad가 불리할까? AdaGrad는 learning rates가 계속 바뀌기 때문이다. 다른 알고리즘은 learning rate가 고정되지만. 

