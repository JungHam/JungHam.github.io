# CS231n-6

## Overview

1. one time setup 
2. Training dynamics 
3. Evaluation

## Activation functions

sigmoid function의 출력이 0이 중심이 아닌게 문제라고?

모든 input이 양수면? gradient가 모두 양수 혹은 음수가 된다. => 매우 비효율적. 전부 같은 방향으로 움직이기 때문이다. 

그리고 exp()의 계산비용이 비싸다. (별로 큰 문제는 아님)



이걸 해결하기 위해서 tanh(x)가 나왔다.

-> zero centered, but saturation때문에 값이 너무 크거나 작으면 기울기가 죽어서 전파가 제대로 되지 않는다. 

ReLu의 경우는 f(x) = max(0,x) 인데, saturate는 아니고(양의 범위에서) 계산도 효율적이고 sigmoid나 tanh보다 계산속도가 빠르다. 수렴속도도 빠르다. 6배 정도. 그리고 생물학적으로 ReLu가 더 타당하다. 역치를 담은 형태이기 때문이다. 하지만 문제점은 여전히 zero-centered가 아니고 음의 수가 input으로 들어오면 gradient가 0이기 때문에 ReLu는 input에 따라 0 이하의 입력값에 대해 gradient를 반영하지 않는다. 그래서 전체 데이터 중에서 선택적으로만 반영함을 알 수 있다. 데이터값에 따라 반영이 안될 수도 있다. 그러면 뭐...gradient도 갱신이 제대로 안되서 수렴이 잘 일어나지 않겠지. 실제로 학습시켜놓은 네트워크를 살펴보면 10-20퍼센트는 수렴을 멈춘 상태라고 한다. (dead ReLu) 결국 데이터의 특성을 잘 반영하지 못한 초평면이 생긴다는 얘기. 

그래서 실제로 ReLu를 초기화할 때, bias를 추가해서 이런 경우를 방지하는 경우도 있지만, 효과가 있는지는 찬반이 갈린다. 

그래서 Leaky ReLu에서 음의 영역에서 살짝 기울기를 주게 되면서 이를 방지하는 경우가 있다.

Maxout Neuron : ReLu와 Leaky ReLU를 보편화한 것. 

실제로 가장 많이 쓰는 것은 ReLu이다. 다만 Learning rate 조심할 것. Sigmoid는 안 쓰기를 권장한다. 



## Data preprocess

zero-centered -> normalized 로 데이터 전처리를 해준다. 이미지의 경우 정규화가 필요하진 않다. 

이미지에서 전처리 하는 경우 

크기를 맞춰주고, 채널 전체의 평균 혹은 각 채널의 평균을 구해서 정규화할 수도 있다. 
PCA나 Whitening기법. 



## Weight Initialization



1. 임의의 작은 값으로 초기화하는 것. 초기 W의 경우 분포에서 값을 가져와서 스케일링한다. : 깊이가 깊은 네트워크에서는 -1 혹은 1로 수렴해서 기울기가 0에 수렴한다. 
2. 정규분포에 루트를 씌워주는 경우(자비에) 

## Batch Normalization

FC나 Convolutional layer 다음에 넣는다. 

입력, mini-batch에서의 평균을 구한다. 병균과 분산으로 정규화하고 scaling등을 한다. 

CNN의 방식은 데이터의 '공간적인 위치'를 잘 보존한다.