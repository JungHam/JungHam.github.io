<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="ko-KR"><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" hreflang="ko-KR" /><updated>2021-02-19T17:14:33+09:00</updated><id>/feed.xml</id><title type="html">Hello, UserErrorWorld!</title><subtitle>Your Site Description
</subtitle><author><name>Hyemi Jeong</name></author><entry><title type="html">[aiffel] Day 36</title><link href="/2021/02/19/AIFFEL-DAY-36.html" rel="alternate" type="text/html" title="[aiffel] Day 36" /><published>2021-02-19T00:00:00+09:00</published><updated>2021-02-19T00:00:00+09:00</updated><id>/2021/02/19/%5BAIFFEL%5D-DAY-36</id><content type="html" xml:base="/2021/02/19/AIFFEL-DAY-36.html">&lt;h1 id=&quot;지시딥-준비&quot;&gt;지시딥 준비&lt;/h1&gt;

&lt;h2 id=&quot;부제--밑바닥부터-시작하는-딥러닝-1권-5장&quot;&gt;부제 : 밑바닥부터 시작하는 딥러닝 1권 5장&lt;/h2&gt;

&lt;h3 id=&quot;사전학습&quot;&gt;사전학습&lt;/h3&gt;

&lt;p&gt;일단 앞 부분을 다시 듣고 수식 전개를 해야할 것 같다. 이 부분이 어렵게 느껴지는건 당연하다고 하니 다행이다. 그리고 효율적인 전파와 역전파 계산을 위해 중간 결과값들을 캐시하는게 맞았다. 코드로 어떻게 구현하는 지가 궁금했는데 이렇게 하는거구나…&lt;/p&gt;

&lt;h3 id=&quot;역전파&quot;&gt;역전파&lt;/h3&gt;

&lt;p&gt;역전파가 하는 일은 연쇄 법칙의 원리와 같다&lt;/p&gt;

&lt;p&gt;덧셈 노드의 경우는 미분하면 1이 나와서 1을 역전파로 전해진 값에 곱하는 거라 입력된 값을 그대로 다음 노드로 전하게 된다.&lt;/p&gt;

&lt;p&gt;곱셈 노드의 경우 상류의 값에 순전파 때의 입력 신호들을 서로 바꾼 값을 곱해서 하류로 보낸다. 미분할때 다른 변수는 상수로 취급하니까.&lt;/p&gt;

&lt;p&gt;#### ReLU&lt;/p&gt;

&lt;p&gt;미분하면 입력값이 0보다 클때 1, 0 이하면 0이라서 0 이하로 신호가 들어오면 역전파 때는 더 이상 신호가 전파되지 않는다. (0을 보내버림). 전기 회로의 ‘스위치’ 에 비유할 수 있다고.&lt;/p&gt;

&lt;h4 id=&quot;sigmoid-계층&quot;&gt;Sigmoid 계층&lt;/h4&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;연쇄법칙에 따라서 정리하면 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;델타y(1-y)&lt;/code&gt;로 정리된다. 와…! 순전파의 출력(y)만으로 계산할 수 있겠군.&lt;/p&gt;

&lt;h4 id=&quot;affine&quot;&gt;Affine&lt;/h4&gt;

&lt;p&gt;신경망의 순전파 때 수행하는 행렬의 곱을 기하학에서는 affine 변환이라고 부른다.&lt;/p&gt;

&lt;p&gt;해당 책의 코드에서는 X * W + B = O로 표현하고 있다. 앞의 계산 그래프들에서는 단일 실수값을 통과시켰는데 이제는 계산 그래프 간에 행렬이 지나가는 것이다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;더 알아볼 점 : 행렬이 끼여 있을 때의 미분&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Hyemi Jeong</name></author><summary type="html">지시딥 준비 부제 : 밑바닥부터 시작하는 딥러닝 1권 5장 사전학습 일단 앞 부분을 다시 듣고 수식 전개를 해야할 것 같다. 이 부분이 어렵게 느껴지는건 당연하다고 하니 다행이다. 그리고 효율적인 전파와 역전파 계산을 위해 중간 결과값들을 캐시하는게 맞았다. 코드로 어떻게 구현하는 지가 궁금했는데 이렇게 하는거구나… 역전파 역전파가 하는 일은 연쇄 법칙의 원리와 같다 덧셈 노드의 경우는 미분하면 1이 나와서 1을 역전파로 전해진 값에 곱하는 거라 입력된 값을 그대로 다음 노드로 전하게 된다. 곱셈 노드의 경우 상류의 값에 순전파 때의 입력 신호들을 서로 바꾼 값을 곱해서 하류로 보낸다. 미분할때 다른 변수는 상수로 취급하니까. #### ReLU 미분하면 입력값이 0보다 클때 1, 0 이하면 0이라서 0 이하로 신호가 들어오면 역전파 때는 더 이상 신호가 전파되지 않는다. (0을 보내버림). 전기 회로의 ‘스위치’ 에 비유할 수 있다고. Sigmoid 계층 y = 1 / (1 + exp(-x)) 연쇄법칙에 따라서 정리하면 델타y(1-y)로 정리된다. 와…! 순전파의 출력(y)만으로 계산할 수 있겠군. Affine 신경망의 순전파 때 수행하는 행렬의 곱을 기하학에서는 affine 변환이라고 부른다. 해당 책의 코드에서는 X * W + B = O로 표현하고 있다. 앞의 계산 그래프들에서는 단일 실수값을 통과시켰는데 이제는 계산 그래프 간에 행렬이 지나가는 것이다. 더 알아볼 점 : 행렬이 끼여 있을 때의 미분</summary></entry><entry><title type="html">[aiffel] Day 34</title><link href="/2021/02/17/AIFFEL-DAY-34.html" rel="alternate" type="text/html" title="[aiffel] Day 34" /><published>2021-02-17T00:00:00+09:00</published><updated>2021-02-17T00:00:00+09:00</updated><id>/2021/02/17/%5BAIFFEL%5D-DAY-34</id><content type="html" xml:base="/2021/02/17/AIFFEL-DAY-34.html">&lt;h1 id=&quot;코딩마스터&quot;&gt;코딩마스터&lt;/h1&gt;

&lt;h2 id=&quot;프로그래머스&quot;&gt;프로그래머스&lt;/h2&gt;

&lt;h3 id=&quot;12930&quot;&gt;12930&lt;/h3&gt;

&lt;p&gt;와..! 아니 이거 조심하자&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Line1-abcdef &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Line2-abc &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Line4-abcd&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;' '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이거 돌리면 어떻게 되게?&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Line1-abcdef'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Line2-abc'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Line4-abcd'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Line1-abcdef'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Line2-abc &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Line4-abcd'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;딜리미터가…사라집니다…코드는 ‘명확히’ 짤 것.&lt;/p&gt;

&lt;h3 id=&quot;42577&quot;&gt;42577&lt;/h3&gt;

&lt;p&gt;이거는 풀이가 여러 방면으로 나눌 것 같은데, 일단 문제조건을 보면 ‘어떤 번호가 다른 번호의 접두사가 되는 경우’라고 해서 엄청 복잡하게 생각했다. 한 번호가 여러 번호의 접두사가 될 건데, 어떻게 캐치해내지? 시간 복잡도는? 정리하면서 생각한 건데 완전탐색이 가장 일반적으로 생각할 수 있는 답이겠구나 싶다.&lt;/p&gt;

&lt;p&gt;그래서 완전탐색을 하되, 좀 더 효율적으로 할 수 있는 방법을 생각했다.  길이별로 먼저 전화번호를 정렬해서 앞의 원소부터 다른 원소의 일부가 되는지를 확인하는 방법이 있었다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;solution&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;phone_book&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;phone_book&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;answer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;phone_book&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; 
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;answer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; 
            &lt;span class=&quot;k&quot;&gt;break&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;phone_book&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;phone_book&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;startswith&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;phone_book&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# 이 짓을 할 게 아니라 걍 phone_book[j]가 string이에요 선생님!
&lt;/span&gt;                    &lt;span class=&quot;n&quot;&gt;answer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;
                    &lt;span class=&quot;k&quot;&gt;break&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;answer&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# 효율성 테스트 1.17ms
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이게…되네? 여기서  나는 숫자를 글자로 바꾸고 앞 부분을 비교하는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;startswith&lt;/code&gt;를 사용했다.&lt;/p&gt;

&lt;p&gt;**정정 : ** 전화번호부가 String으로 들어오기 때문에, 중간 과정이 필요없었다. python에서 sort를 쓰면 기본적으로 사전순으로 소팅을 한다.&lt;/p&gt;

&lt;h2 id=&quot;백준&quot;&gt;백준&lt;/h2&gt;

&lt;p&gt;ㅎㅎㅎ 백준에서 파이썬을 쓴게 처음이라서, 제출 시도부터가 글렀다. 그리고 의외로 python sort가 cost가 크지 않은게 신기했다. C++에서는 모든 연산에서 cost를 따졌으니까^^&lt;/p&gt;

&lt;p&gt;그래서 이번 문제는 어차피 C++로 풀었지만, 다시 한번 python으로 풀면서 이번에는 백준을 쓰면 어떻게 파이썬을 제출해야 하는지 알아보겠습니다.&lt;/p&gt;

&lt;h4 id=&quot;입력은-어떻게-받을-것인가&quot;&gt;입력은 어떻게 받을 것인가&lt;/h4&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sys&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stdin&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# 입력받을 것이 하나
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stdin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;readline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# 입력받을 것이 여러개
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;li&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stdin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;readline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;아 왜 못 풀었는지 알겠다…이거ㅠㅠㅠ 입력받은거 list로 감싸줘야 하는데 map하고 냅둬서 index로 참조를 못하는 상황이었다.&lt;/p&gt;

&lt;p&gt;이번에 사용한 온라인 인터프리터는 이거 : https://repl.it/languages/python3&lt;/p&gt;</content><author><name>Hyemi Jeong</name></author><summary type="html">코딩마스터 프로그래머스 12930 와..! 아니 이거 조심하자 str = &quot;Line1-abcdef \nLine2-abc \nLine4-abcd&quot;; print str.split( ) print str.split(' ', 1 ) 이거 돌리면 어떻게 되게? ['Line1-abcdef', 'Line2-abc', 'Line4-abcd'] ['Line1-abcdef', '\nLine2-abc \nLine4-abcd'] 딜리미터가…사라집니다…코드는 ‘명확히’ 짤 것. 42577 이거는 풀이가 여러 방면으로 나눌 것 같은데, 일단 문제조건을 보면 ‘어떤 번호가 다른 번호의 접두사가 되는 경우’라고 해서 엄청 복잡하게 생각했다. 한 번호가 여러 번호의 접두사가 될 건데, 어떻게 캐치해내지? 시간 복잡도는? 정리하면서 생각한 건데 완전탐색이 가장 일반적으로 생각할 수 있는 답이겠구나 싶다. 그래서 완전탐색을 하되, 좀 더 효율적으로 할 수 있는 방법을 생각했다. 길이별로 먼저 전화번호를 정렬해서 앞의 원소부터 다른 원소의 일부가 되는지를 확인하는 방법이 있었다. def solution(phone_book): phone_book.sort(key=len) answer = True; for i in range(0, len(phone_book)) : if answer == False : break for j in range(i+1, len(phone_book)) : if str(phone_book[j]).startswith(str(phone_book[i])) : # 이 짓을 할 게 아니라 걍 phone_book[j]가 string이에요 선생님! answer = False break return answer # 효율성 테스트 1.17ms 이게…되네? 여기서 나는 숫자를 글자로 바꾸고 앞 부분을 비교하는 startswith를 사용했다. **정정 : ** 전화번호부가 String으로 들어오기 때문에, 중간 과정이 필요없었다. python에서 sort를 쓰면 기본적으로 사전순으로 소팅을 한다. 백준 ㅎㅎㅎ 백준에서 파이썬을 쓴게 처음이라서, 제출 시도부터가 글렀다. 그리고 의외로 python sort가 cost가 크지 않은게 신기했다. C++에서는 모든 연산에서 cost를 따졌으니까^^ 그래서 이번 문제는 어차피 C++로 풀었지만, 다시 한번 python으로 풀면서 이번에는 백준을 쓰면 어떻게 파이썬을 제출해야 하는지 알아보겠습니다. 입력은 어떻게 받을 것인가 from sys import stdin # 입력받을 것이 하나 n = int(stdin.readline()) # 입력받을 것이 여러개 li = list(map(int, stdin.readline().split())) 아 왜 못 풀었는지 알겠다…이거ㅠㅠㅠ 입력받은거 list로 감싸줘야 하는데 map하고 냅둬서 index로 참조를 못하는 상황이었다. 이번에 사용한 온라인 인터프리터는 이거 : https://repl.it/languages/python3</summary></entry><entry><title type="html">[aiffel] Day 32</title><link href="/2021/02/15/AIFFEL-DAY-32.html" rel="alternate" type="text/html" title="[aiffel] Day 32" /><published>2021-02-15T00:00:00+09:00</published><updated>2021-02-15T00:00:00+09:00</updated><id>/2021/02/15/%5BAIFFEL%5D-DAY-32</id><content type="html" xml:base="/2021/02/15/AIFFEL-DAY-32.html">&lt;h1 id=&quot;cs231n-7강&quot;&gt;CS231n 7강&lt;/h1&gt;

&lt;p&gt;저번시간 복습&lt;/p&gt;

&lt;p&gt;internal covariant : 데이터 간의 분포가 달라지면 학습 결과가 달라질 수 있다.&lt;/p&gt;

&lt;p&gt;Batch Normalization의 위치 : non-linearble 연산이 필요한 경우&lt;/p&gt;

&lt;p&gt;그래서 normalization하는 단계는 2단계(normalization -&amp;gt; network 학습 변수 적용)&lt;/p&gt;

&lt;p&gt;모멘텀 기법 + gradient squared term&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;vx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compute_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rho&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vx&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;여기서 rho값이 velocity의 영향력을 조절하는데, 보통 0.9, 0.99와 같이 높은 값을 준다. 그래서 gradient vector가 바로 반영되는게 아니라 velocity의 영향을 받아서 나아가게 된다.&lt;/p&gt;

&lt;h3 id=&quot;adam&quot;&gt;Adam&lt;/h3&gt;

&lt;p&gt;bias correction에 대해서 좀 더 서술하자.&lt;/p&gt;

&lt;p&gt;momentum + ada = adam&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;first_moment&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;second_moment&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_iterations&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compute_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# for momentum
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;first_moment&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;first_moment&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# for RMSProp
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;second_moment&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;second_moment&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# bias correction start
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;first_unbias&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;first_moment&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;second_unbias&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;second_moment&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# bias correction end
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# AdaGrad / RMSProp
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;first_unbias&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;second_unbias&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;여기서 beta2 = 0.999, learning_rate = 1e-3 또는 5e-4&lt;/p&gt;

&lt;p&gt;첫째 둘째 모멘트를 계산하고 난 다음에 그 둘에서 unbias 값을 계산한 후, 이를 다음 스텝에서 적용한다. (베타, 알파)&lt;/p&gt;

&lt;p&gt;만약 unbias를 계산하지 않으면 다음의 문제가 생긴다. first_moment와 second_moment가 0에서 시작하는데, second_moment는 첫번째 스텝이 지나가도 0에 수렴하게 된다. 이를 보정하기 위해 Bias correction을 한다.&lt;/p&gt;

&lt;p&gt;처음에는 learning rate를 높여서 학습하다가 그 이후에는 꾸준히 낮추면서 학습한다. (learning rate decay)&lt;/p&gt;

&lt;p&gt;이 기법은 adam보다 sgd에서 더 많이 쓰이고, 부차적인 하이퍼파라미터가 된다. 정확도를 지켜보다가 필요하다 싶을 때 learning rate를 변경하면서.&lt;/p&gt;

&lt;p&gt;First-order optimization : 그 시점에서의 접선의 기울기를 구하는 방법 -&amp;gt;  second-order optimization이 나온듯.&lt;/p&gt;

&lt;p&gt;1차 미분은 기울기, 2차 미분은 기울기의 양상을 구할수 있다!&lt;/p&gt;

&lt;p&gt;Second-order optimization : 2차 미분의 양상을 사용한다. First-order를 사용하는 것보다 좋긴 한데, 사용할 수 있는 곳이 정해져 있다. Newton step. Hessian matrix. 여기서는 learning rate가 없다. 하이퍼파라미터도 없다. Hessian은 N^3를 사용하기 때문에, 딥러닝에서 활용할 수가 없다(무수한 차원을 생각해보라)&lt;/p&gt;

&lt;p&gt;그래서 Quasi-Newton에서는 N^2로 줄지만 이래도 딥러닝에 활용하기에는 너무 크다. 만약 full-batch를 시도할거면, L-BFGS를 사용해봐라. (하지만 Adam을 쓰세요…)&lt;/p&gt;

&lt;h4 id=&quot;beyond-training-error&quot;&gt;Beyond Training Error&lt;/h4&gt;

&lt;p&gt;regularization에 대해서 다뤄볼 에정.&lt;/p&gt;

&lt;p&gt;train과 test(validation)의 학습 결과를 줄이기 위해서 어떤 것을 해야 하나? : 독립적인 여러 모델 쓰고 앙상블 기법을 써서 결과의 평균을 사용하기. 근데 2프로 정도 증가한다고… (ex : imagenet)&lt;/p&gt;

&lt;p&gt;좀 더 나아가면 모델들을 저장하고(snapshot) 앙상블로 사용해볼 수 있다. 학습 중간중간에 앙상블의 결과를 보고 이를 학습에 반영한다. 그래서 learning rate를 높였다 낮췄다 퐁당퐁당하면서 지나간다.&lt;/p&gt;

&lt;p&gt;만약 평균을 낼 때 모델 간의 결과의 gap을 신경쓰지 않는다면 validation에 overfitting하면 되지 않을까 라는 단순한 원리로 접근할 수도 있다. poiyak averaging이라는 기법이 있다.&lt;/p&gt;

&lt;p&gt;단일 모델의 성능을 향상시키려면 어떻게 해야할까?&lt;/p&gt;

&lt;h4 id=&quot;regularization--add-term-to-loss&quot;&gt;Regularization : Add term to loss&lt;/h4&gt;

&lt;p&gt;Training할 때 random noise를 더하고, testing때는 noise를 평균화.&lt;/p&gt;

&lt;p&gt;L1, L2, Elastic net&lt;/p&gt;

&lt;p&gt;Fully connected layer -&amp;gt; 어떤 뉴런들을 렌덤하게 반영하지 않는다. 근데 너무 많은 특징을 반영하게 되면 랜덤하게 누락된 뉴런이 특징을 파악할 때 관여하는 경우 불리할 수 있다.&lt;/p&gt;

&lt;p&gt;Dropout : Random mask를 쓰는 방법 (&amp;lt;- 후에 복습하기!)&lt;/p&gt;

&lt;p&gt;그렇다면 이를 test time에서 어떻게 dropout을 사용할까? Dropout한 만큼 test time에서 값을 증폭해야 한다. 하지만 inverted dropout이라는 것도 있는데 이건 train할때 떨군 확률 p를 나눠주고, test time에서는 이를 그냥 둔다.&lt;/p&gt;

&lt;p&gt;average out —&amp;gt; 이거 나중에 복습하기!&lt;/p&gt;

&lt;p&gt;dropout이 적용되지 않은 노드에서만 back propagation이 일어난다. 결국 Reularization은 네트워크에 무작위성을 추가해서 train 데이터에 overfitting하는 문제를 막는다.  BN이랑 비슷한 효과를 얻을 수 있다. Dropout은 BN과 달리 자유롭게 조정할 수 있는 파라미터 p(drop 확률)가 존재한다.&lt;/p&gt;

&lt;h4 id=&quot;data-augmentation&quot;&gt;Data Augmentation&lt;/h4&gt;

&lt;p&gt;이미지의 일부를 잘라서 변형시켜 이를 test에 사용할 수 있다. 그리고 이 외에도 다양한 방법이 있는데, 핵심은 label을 유지하면서 데이터에 변형을 가하는 방법이다.&lt;/p&gt;

&lt;p&gt;Fractional maxpooling : pooling할 지역이 random하게 지정된다.&lt;/p&gt;

&lt;p&gt;stochastic depth에 대해서 : 곁보기에는 바로 직전의 결과물이 아닌 몇 스텝 이전의 결과물을 그 다음에 반영하는 걸로 보이는데…&lt;/p&gt;

&lt;p&gt;보통은 BN을 많이 쓰지만 overfitting이 해결되지 않는다면 Dropout과 같은 다양한 방법을 추가해볼 수 있다.&lt;/p&gt;

&lt;p&gt;transfer learning(꽤 유용한 방법) -&amp;gt; 해커톤 같은 경우에 사용가능하다.&lt;/p&gt;

&lt;p&gt;데이터가 얼마 없는 경우에 transfer learning을 사용한다. Fully connected layer쪽을 다시 학습시키는 방법을 사용한다. 비슷한 데이터셋의 경우에는 데이터셋 크기 작으면 선형 분류, 많으면 몇몇 레이어를 잘 튜닝시키고 다른 데이터들이 있는 데이터셋의 경우에는 다르게 대처한다.&lt;/p&gt;

&lt;p&gt;데이터가 1M 이하면 transfer learning을 추천한다.&lt;/p&gt;

&lt;h1 id=&quot;cs231n-09&quot;&gt;CS231n-09&lt;/h1&gt;

&lt;p&gt;CNN output size calculator&lt;/p&gt;

&lt;p&gt;you can use this formula &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[(W−K+2P)/S]+1&lt;/code&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;W is the input volume - in your case 128&lt;/li&gt;
  &lt;li&gt;K is the Kernel size - in your case 5&lt;/li&gt;
  &lt;li&gt;P is the padding - in your case 0 i believe&lt;/li&gt;
  &lt;li&gt;S is the stride - which you have not provided.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;파라미터의 수는 96 * 11 * 11 * 3&lt;/p&gt;

&lt;p&gt;pooling layer에는 parameter가 없다. 왜냐하면 파라미터는 우리가 학습하는 가중치를 의미하는데, pooling의 연산은 학습하면서 적용할 가중치가 없기 때문이다.&lt;/p&gt;

&lt;p&gt;AlexNet의 경우 모델을 2개로 나누어서 학습했는데 이유는 GPU 메모리 용량때문에(…)&lt;/p&gt;

&lt;p&gt;VGGNet에 대해서 설명 :&lt;/p&gt;

&lt;p&gt;Small filters, Deeper networks.&lt;/p&gt;

&lt;p&gt;왜 크기가 작은 필터를 사용했나? 크기가 작은 필터를 여러 레이어에 걸쳐 적용한 것은 크기가 큰 필터를 한번 적용한 것과 같다. ( ex : 3x3 필터 세 번과 7x7 필터 한번 )&lt;/p&gt;</content><author><name>Hyemi Jeong</name></author><summary type="html">CS231n 7강 저번시간 복습 internal covariant : 데이터 간의 분포가 달라지면 학습 결과가 달라질 수 있다. Batch Normalization의 위치 : non-linearble 연산이 필요한 경우 그래서 normalization하는 단계는 2단계(normalization -&amp;gt; network 학습 변수 적용) 모멘텀 기법 + gradient squared term vx = 0 while True : dx = compute_gradient(x) vx = rho * vx + dx x += learning_rate * vx 여기서 rho값이 velocity의 영향력을 조절하는데, 보통 0.9, 0.99와 같이 높은 값을 준다. 그래서 gradient vector가 바로 반영되는게 아니라 velocity의 영향을 받아서 나아가게 된다. Adam bias correction에 대해서 좀 더 서술하자. momentum + ada = adam first_moment = 0 second_moment = 0 for i in range(1, num_iterations) : dx = compute_gradient(x) # for momentum first_moment = beta1 * first_moment + (1-beta1) * dx # for RMSProp second_moment = beta2 * second_moment + (1-beta2) * dx * dx # bias correction start first_unbias = first_moment / (1-beta1**t) second_unbias = second_moment / (1-beta2**t) # bias correction end # AdaGrad / RMSProp x -= learning_rate * first_unbias / (np.sqrt(second_unbias) + 1e-7) 여기서 beta2 = 0.999, learning_rate = 1e-3 또는 5e-4 첫째 둘째 모멘트를 계산하고 난 다음에 그 둘에서 unbias 값을 계산한 후, 이를 다음 스텝에서 적용한다. (베타, 알파) 만약 unbias를 계산하지 않으면 다음의 문제가 생긴다. first_moment와 second_moment가 0에서 시작하는데, second_moment는 첫번째 스텝이 지나가도 0에 수렴하게 된다. 이를 보정하기 위해 Bias correction을 한다. 처음에는 learning rate를 높여서 학습하다가 그 이후에는 꾸준히 낮추면서 학습한다. (learning rate decay) 이 기법은 adam보다 sgd에서 더 많이 쓰이고, 부차적인 하이퍼파라미터가 된다. 정확도를 지켜보다가 필요하다 싶을 때 learning rate를 변경하면서. First-order optimization : 그 시점에서의 접선의 기울기를 구하는 방법 -&amp;gt; second-order optimization이 나온듯. 1차 미분은 기울기, 2차 미분은 기울기의 양상을 구할수 있다! Second-order optimization : 2차 미분의 양상을 사용한다. First-order를 사용하는 것보다 좋긴 한데, 사용할 수 있는 곳이 정해져 있다. Newton step. Hessian matrix. 여기서는 learning rate가 없다. 하이퍼파라미터도 없다. Hessian은 N^3를 사용하기 때문에, 딥러닝에서 활용할 수가 없다(무수한 차원을 생각해보라) 그래서 Quasi-Newton에서는 N^2로 줄지만 이래도 딥러닝에 활용하기에는 너무 크다. 만약 full-batch를 시도할거면, L-BFGS를 사용해봐라. (하지만 Adam을 쓰세요…) Beyond Training Error regularization에 대해서 다뤄볼 에정. train과 test(validation)의 학습 결과를 줄이기 위해서 어떤 것을 해야 하나? : 독립적인 여러 모델 쓰고 앙상블 기법을 써서 결과의 평균을 사용하기. 근데 2프로 정도 증가한다고… (ex : imagenet) 좀 더 나아가면 모델들을 저장하고(snapshot) 앙상블로 사용해볼 수 있다. 학습 중간중간에 앙상블의 결과를 보고 이를 학습에 반영한다. 그래서 learning rate를 높였다 낮췄다 퐁당퐁당하면서 지나간다. 만약 평균을 낼 때 모델 간의 결과의 gap을 신경쓰지 않는다면 validation에 overfitting하면 되지 않을까 라는 단순한 원리로 접근할 수도 있다. poiyak averaging이라는 기법이 있다. 단일 모델의 성능을 향상시키려면 어떻게 해야할까? Regularization : Add term to loss Training할 때 random noise를 더하고, testing때는 noise를 평균화. L1, L2, Elastic net Fully connected layer -&amp;gt; 어떤 뉴런들을 렌덤하게 반영하지 않는다. 근데 너무 많은 특징을 반영하게 되면 랜덤하게 누락된 뉴런이 특징을 파악할 때 관여하는 경우 불리할 수 있다. Dropout : Random mask를 쓰는 방법 (&amp;lt;- 후에 복습하기!) 그렇다면 이를 test time에서 어떻게 dropout을 사용할까? Dropout한 만큼 test time에서 값을 증폭해야 한다. 하지만 inverted dropout이라는 것도 있는데 이건 train할때 떨군 확률 p를 나눠주고, test time에서는 이를 그냥 둔다. average out —&amp;gt; 이거 나중에 복습하기! dropout이 적용되지 않은 노드에서만 back propagation이 일어난다. 결국 Reularization은 네트워크에 무작위성을 추가해서 train 데이터에 overfitting하는 문제를 막는다. BN이랑 비슷한 효과를 얻을 수 있다. Dropout은 BN과 달리 자유롭게 조정할 수 있는 파라미터 p(drop 확률)가 존재한다. Data Augmentation 이미지의 일부를 잘라서 변형시켜 이를 test에 사용할 수 있다. 그리고 이 외에도 다양한 방법이 있는데, 핵심은 label을 유지하면서 데이터에 변형을 가하는 방법이다. Fractional maxpooling : pooling할 지역이 random하게 지정된다. stochastic depth에 대해서 : 곁보기에는 바로 직전의 결과물이 아닌 몇 스텝 이전의 결과물을 그 다음에 반영하는 걸로 보이는데… 보통은 BN을 많이 쓰지만 overfitting이 해결되지 않는다면 Dropout과 같은 다양한 방법을 추가해볼 수 있다. transfer learning(꽤 유용한 방법) -&amp;gt; 해커톤 같은 경우에 사용가능하다. 데이터가 얼마 없는 경우에 transfer learning을 사용한다. Fully connected layer쪽을 다시 학습시키는 방법을 사용한다. 비슷한 데이터셋의 경우에는 데이터셋 크기 작으면 선형 분류, 많으면 몇몇 레이어를 잘 튜닝시키고 다른 데이터들이 있는 데이터셋의 경우에는 다르게 대처한다. 데이터가 1M 이하면 transfer learning을 추천한다. CS231n-09 CNN output size calculator you can use this formula [(W−K+2P)/S]+1. W is the input volume - in your case 128 K is the Kernel size - in your case 5 P is the padding - in your case 0 i believe S is the stride - which you have not provided. 파라미터의 수는 96 * 11 * 11 * 3 pooling layer에는 parameter가 없다. 왜냐하면 파라미터는 우리가 학습하는 가중치를 의미하는데, pooling의 연산은 학습하면서 적용할 가중치가 없기 때문이다. AlexNet의 경우 모델을 2개로 나누어서 학습했는데 이유는 GPU 메모리 용량때문에(…) VGGNet에 대해서 설명 : Small filters, Deeper networks. 왜 크기가 작은 필터를 사용했나? 크기가 작은 필터를 여러 레이어에 걸쳐 적용한 것은 크기가 큰 필터를 한번 적용한 것과 같다. ( ex : 3x3 필터 세 번과 7x7 필터 한번 )</summary></entry><entry><title type="html">[aiffel] Day 29</title><link href="/2021/02/08/AIFFEL-DAY-29.html" rel="alternate" type="text/html" title="[aiffel] Day 29" /><published>2021-02-08T00:00:00+09:00</published><updated>2021-02-08T00:00:00+09:00</updated><id>/2021/02/08/%5BAIFFEL%5D-DAY-29</id><content type="html" xml:base="/2021/02/08/AIFFEL-DAY-29.html">&lt;p&gt;항상 gem과 jekyll이 문제다. 분명 그전날 세팅 다 했는데 다음날 충돌하는 모양을 보고 있노라면, 매우 허무해진다. 그래서 ruby부터 다시 깔았으므로, 다음에 충돌이 날 때는 어쩔 수 없다. PATH 확인하고, bundle이 의존하는 ruby를 에러메세지에서 주의깊게 살펴본 후 도저히 안되면 날려버리고 새로 세팅하라.&lt;/p&gt;

&lt;h1 id=&quot;cs231n-06&quot;&gt;CS231n-06&lt;/h1&gt;

&lt;h2 id=&quot;batch-normalization&quot;&gt;Batch normalization&lt;/h2&gt;

&lt;p&gt;배치로 얻은 데이터가 나왔을 때, 각 배치의 평균과 분산을 통해 normalization 수행.&lt;/p&gt;

&lt;p&gt;각 차원에서 평균과 분산을 구하고, 여기서 normalization을 수행한다. 보통 FC나 Convolutional layers 이후, 혹은 비선형 단계 전에 행한다.&lt;/p&gt;

&lt;p&gt;근데 activation function의 input이 반드시 가우시안 분포를 따라야 하는가? 우리의 목표는 activation function에 들어오는 input이 의미있는 activation function의 output값을 가지도록 처리하는 것이다.&lt;/p&gt;

&lt;p&gt;CNN에서는 activation map에서 데이터의 공간적인 관계를 유지하길 원하니까, 각 activation map간의 평균과 분산을 구해서 normalization 을 시도한다.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;((layer input)-평균)/(분산) = 가우시안 분포&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;평균을 빼면 데이터가 0을 원점으로 위치하게 되고 분산으로 나누게 되면 데이터들이 0과 1사이에 분포하게 된다.&lt;/p&gt;

&lt;h2 id=&quot;babysitting-the-learning-process&quot;&gt;Babysitting the learning process&lt;/h2&gt;

&lt;p&gt;학습 과정이 어떻게 이루어지는지 보고 하이퍼파라미터를 어떻게 조정할 것인지에 대해서 배울 것.&lt;/p&gt;

&lt;p&gt;네트워크를 초기화한다. foward pass를 시켜보고 loss가 제대로 이루어지는지 확인한다. 그리고 데이터의 일부만 학습시켜 본다. Overfit과 Loss가 생길거다. 이 때는 regularization을 하지 않는다. 그리고 아주 조금의 regularization을 적용해본다. 이렇게 조금씩 값을 바꿀 경우 이 상황의 경우 결과 값들의 확률이 고르게 분포하고 있고, accuracy는 이 비슷비슷한 값 중에서 제일 큰 값을 취하기 때문에 accuracy가 갑자기 커질 수 있다. learning rate를 키우게 되면 cost가 어떻게 되나? 너무 키우면 cost가 발산해 버린다.&lt;/p&gt;

&lt;h2 id=&quot;hyperparameter-optimization&quot;&gt;Hyperparameter Optimization&lt;/h2&gt;

&lt;p&gt;처음에는 파라미터의 효율성을 알기 위해서 적은 횟수의 학습을 돌린다. 두 번째에서는 더 많이 반복해서 더 정제된 결과를 얻는다. cost가 이전의 3배 이상으로 발산하면 잘못된 파라미터를 가지고 학습하고 있는 것. validation accuracy를 넣어서 학습하는데, 파라미터 값은 log scale의 값으로 샘플링하자. (이유 후술)&lt;/p&gt;

&lt;p&gt;grid search는 어떤가? 하이퍼파라미터를 일정한 간격으로 증감시켜 파라미터를 넣어보는 것이다. 실제로는 random search가 더 잘 작동하지만.&lt;/p&gt;

&lt;p&gt;validation accuracy는 높아지지 않는다.&lt;/p&gt;

&lt;h1 id=&quot;cs231n-07&quot;&gt;CS231n-07&lt;/h1&gt;

&lt;h2 id=&quot;fancier-optimization&quot;&gt;Fancier optimization&lt;/h2&gt;

&lt;p&gt;손실 함수가 가중치에 대한 landscape라고 생각하자. gradient descent를 줄여나가는 과정에서, 기존의 SGD의 경우 목표축이 아닌 축의 방향에 대해서는 지그재그로 값이 흔들리면서 느리게 업데이트 되는 것을 볼 수 있다. 그리고 SGD에서 발생하는 다른 문제점은 local minima와 saddle point이다. 둘 다에 걸려버린다. saddle point는 gradient가 0에 가깝기 때문이다.&lt;/p&gt;

&lt;p&gt;saddle point : 안장점. 변곡점이랑은 차원에 따라 같은 개념으로 취급하는 듯. 3차원으로 가면 2개의 변화지점이 겹치는 지점.&lt;/p&gt;

&lt;h3 id=&quot;부제--어떻게-learning-rate와-step의-크기를-제어할-수-있을까&quot;&gt;부제 : 어떻게 learning rate와 step의 크기를 제어할 수 있을까?&lt;/h3&gt;

&lt;p&gt;Momentum을 추가하면 이러한 흔들림을 해결할 수 있다. 파라미터 하나가 더 추가되는듯. Gradient와 velocity(gradient의 평균값)의 연산을 통해 실제 움직이는 과정을 정할 수 있다. Momentum, Nesterov Momentum. Nesterov Momentum에서는 Loss와 Gradient를 같은 점에서 계산하는 기존의 방식에서 살짝 변형을 주어 이전과 현재의 velocity간의 에러 보정이 들어가있다. Momentum은 단어 그대로 이전까지의 변화량을 바탕으로, 다음 스텝의 움직임에 이 변화량을 반영해 바로 gradient의 변화가 이루어지는게 아닌 이전 방향의 움직임에 영향을 받도록 하는 기법이다.&lt;/p&gt;

&lt;p&gt;근데 만약 minima가 길고 좁을때 그냥 지나치면 어떡해? &amp;gt; 흥미로운 부분이다. 사실 이런 특징을 가진 데이터는 바람직하진 않은데, 그래도 이걸 어떻게 해결할까에는 중요한 연구과제.&lt;/p&gt;

&lt;p&gt;AdaGrad는 훈련 도중의 gradient를 사용한다. 앞서 게산한 gradient를 제곱한 값을 사용한다. 그래서 기울기가 큰 경우에는 속도가 느려지고, 기울기가 작은 경우에는 속도가 빨라진다. 이렇게 되면 학습 횟수가 늘어난다. 손실함수가 convex한 경우에는 minimun에 근접하면서 서서히 속도를 줄이는 게 좋은 현상이 될 수 있지만, non-convex한 케이스에서는 제대로 작동하지 않을 수 있다.&lt;/p&gt;

&lt;p&gt;그래서 RMSProp에서 이러한 문제를 개선한다. AdaGrad는 learning rate의 문제로 잘 사용하지 않는다. 근데 왜 convex에서도 AdaGrad가 불리할까? AdaGrad는 learning rates가 계속 바뀌기 때문이다. 다른 알고리즘은 learning rate가 고정되지만.&lt;/p&gt;</content><author><name>Hyemi Jeong</name></author><summary type="html">항상 gem과 jekyll이 문제다. 분명 그전날 세팅 다 했는데 다음날 충돌하는 모양을 보고 있노라면, 매우 허무해진다. 그래서 ruby부터 다시 깔았으므로, 다음에 충돌이 날 때는 어쩔 수 없다. PATH 확인하고, bundle이 의존하는 ruby를 에러메세지에서 주의깊게 살펴본 후 도저히 안되면 날려버리고 새로 세팅하라. CS231n-06 Batch normalization 배치로 얻은 데이터가 나왔을 때, 각 배치의 평균과 분산을 통해 normalization 수행. 각 차원에서 평균과 분산을 구하고, 여기서 normalization을 수행한다. 보통 FC나 Convolutional layers 이후, 혹은 비선형 단계 전에 행한다. 근데 activation function의 input이 반드시 가우시안 분포를 따라야 하는가? 우리의 목표는 activation function에 들어오는 input이 의미있는 activation function의 output값을 가지도록 처리하는 것이다. CNN에서는 activation map에서 데이터의 공간적인 관계를 유지하길 원하니까, 각 activation map간의 평균과 분산을 구해서 normalization 을 시도한다. ((layer input)-평균)/(분산) = 가우시안 분포 평균을 빼면 데이터가 0을 원점으로 위치하게 되고 분산으로 나누게 되면 데이터들이 0과 1사이에 분포하게 된다. Babysitting the learning process 학습 과정이 어떻게 이루어지는지 보고 하이퍼파라미터를 어떻게 조정할 것인지에 대해서 배울 것. 네트워크를 초기화한다. foward pass를 시켜보고 loss가 제대로 이루어지는지 확인한다. 그리고 데이터의 일부만 학습시켜 본다. Overfit과 Loss가 생길거다. 이 때는 regularization을 하지 않는다. 그리고 아주 조금의 regularization을 적용해본다. 이렇게 조금씩 값을 바꿀 경우 이 상황의 경우 결과 값들의 확률이 고르게 분포하고 있고, accuracy는 이 비슷비슷한 값 중에서 제일 큰 값을 취하기 때문에 accuracy가 갑자기 커질 수 있다. learning rate를 키우게 되면 cost가 어떻게 되나? 너무 키우면 cost가 발산해 버린다. Hyperparameter Optimization 처음에는 파라미터의 효율성을 알기 위해서 적은 횟수의 학습을 돌린다. 두 번째에서는 더 많이 반복해서 더 정제된 결과를 얻는다. cost가 이전의 3배 이상으로 발산하면 잘못된 파라미터를 가지고 학습하고 있는 것. validation accuracy를 넣어서 학습하는데, 파라미터 값은 log scale의 값으로 샘플링하자. (이유 후술) grid search는 어떤가? 하이퍼파라미터를 일정한 간격으로 증감시켜 파라미터를 넣어보는 것이다. 실제로는 random search가 더 잘 작동하지만. validation accuracy는 높아지지 않는다. CS231n-07 Fancier optimization 손실 함수가 가중치에 대한 landscape라고 생각하자. gradient descent를 줄여나가는 과정에서, 기존의 SGD의 경우 목표축이 아닌 축의 방향에 대해서는 지그재그로 값이 흔들리면서 느리게 업데이트 되는 것을 볼 수 있다. 그리고 SGD에서 발생하는 다른 문제점은 local minima와 saddle point이다. 둘 다에 걸려버린다. saddle point는 gradient가 0에 가깝기 때문이다. saddle point : 안장점. 변곡점이랑은 차원에 따라 같은 개념으로 취급하는 듯. 3차원으로 가면 2개의 변화지점이 겹치는 지점. 부제 : 어떻게 learning rate와 step의 크기를 제어할 수 있을까? Momentum을 추가하면 이러한 흔들림을 해결할 수 있다. 파라미터 하나가 더 추가되는듯. Gradient와 velocity(gradient의 평균값)의 연산을 통해 실제 움직이는 과정을 정할 수 있다. Momentum, Nesterov Momentum. Nesterov Momentum에서는 Loss와 Gradient를 같은 점에서 계산하는 기존의 방식에서 살짝 변형을 주어 이전과 현재의 velocity간의 에러 보정이 들어가있다. Momentum은 단어 그대로 이전까지의 변화량을 바탕으로, 다음 스텝의 움직임에 이 변화량을 반영해 바로 gradient의 변화가 이루어지는게 아닌 이전 방향의 움직임에 영향을 받도록 하는 기법이다. 근데 만약 minima가 길고 좁을때 그냥 지나치면 어떡해? &amp;gt; 흥미로운 부분이다. 사실 이런 특징을 가진 데이터는 바람직하진 않은데, 그래도 이걸 어떻게 해결할까에는 중요한 연구과제. AdaGrad는 훈련 도중의 gradient를 사용한다. 앞서 게산한 gradient를 제곱한 값을 사용한다. 그래서 기울기가 큰 경우에는 속도가 느려지고, 기울기가 작은 경우에는 속도가 빨라진다. 이렇게 되면 학습 횟수가 늘어난다. 손실함수가 convex한 경우에는 minimun에 근접하면서 서서히 속도를 줄이는 게 좋은 현상이 될 수 있지만, non-convex한 케이스에서는 제대로 작동하지 않을 수 있다. 그래서 RMSProp에서 이러한 문제를 개선한다. AdaGrad는 learning rate의 문제로 잘 사용하지 않는다. 근데 왜 convex에서도 AdaGrad가 불리할까? AdaGrad는 learning rates가 계속 바뀌기 때문이다. 다른 알고리즘은 learning rate가 고정되지만.</summary></entry><entry><title type="html">[aiffel] Day 22</title><link href="/2021/01/27/AIFFEL-DAY-22.html" rel="alternate" type="text/html" title="[aiffel] Day 22" /><published>2021-01-27T00:00:00+09:00</published><updated>2021-01-27T00:00:00+09:00</updated><id>/2021/01/27/%5BAIFFEL%5D-DAY-22</id><content type="html" xml:base="/2021/01/27/AIFFEL-DAY-22.html">&lt;h1 id=&quot;파이썬을-해봅시다&quot;&gt;파이썬을 해봅시다&lt;/h1&gt;

&lt;p&gt;오전 기초 과정에서는 파이썬 문법 심화, 오후 풀잎 시간에는 파이썬 공부 및 코테 풀어보기(프로그래머스)를 진행했다.&lt;/p&gt;

&lt;p&gt;오늘 주목한 것 : immutable vs mutable&lt;/p&gt;

&lt;p&gt;그래서 코드를 직접 짜봐서 테스트를 해봤다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;make 2 tuple variable&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;tuple&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;assign new tuple at the other tuple&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Is it possible to reassign new tuple to original variable?&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;try&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;How about change the element of tuple&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;except&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;this expression is invaild&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;make 2 tuple variable
139912658793168 139912658793168
assign new tuple at the other tuple
(1, 2, 3)
(1, 2, 3, 4)
139912658793168 139912658652336
Is it possible to reassign new tuple to original variable?
(1, 2, 3, 4)
139912658652912
How about change the element of tuple
this expression is invaild
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;tuple은 ‘변경이 불가능한’ 자료구조다. 실제로 index로 원소에 접근해서 tuple을 조작하려고 할 때, 에러가 난다. 하지만 tuple에 새 원소를 더해서 다른 변수에 할당하는 것은 가능하다. 이로써 파이썬에서의 ‘변수’는 메모리 공간을 가리키는 역할을 하고, 이 메모리 공간에 변수를 사용해서 접근한 후 그 공간을 변경가능한지 여부가 immutable과 mutable을 나눈다고 결론지었다.&lt;/p&gt;

&lt;p&gt;C++에서 리스트의 경우 각 리스트는 포인터의 연속이고, 각 포인터가 가리키는 메모리 공간은 불연속성을 띌 수도 있다고 배웠는데 파이썬의 리스트의 원소들은 메모리를 가리키는 포인터의 연속이고, 이 포인터를 통해 메모리에 접근해서 메모리가 담고 있는 값을 변경가능하다.&lt;/p&gt;

&lt;p&gt;참고 : https://bearwoong.tistory.com/85&lt;/p&gt;

&lt;p&gt;그리고 풀어본 문제는 이것!&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;https://programmers.co.kr/learn/courses/30/lessons/12915&lt;/li&gt;
  &lt;li&gt;https://programmers.co.kr/learn/courses/30/lessons/12916&lt;/li&gt;
  &lt;li&gt;https://programmers.co.kr/learn/courses/30/lessons/64061&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;파이썬 람다식에 대한 공부가 더 필요할 듯 하다. 람다식을 쓰면 정렬을 할 때 기준을 내 마음대로 할 수 있다.&lt;/p&gt;

&lt;p&gt;그리고 파이썬 빈 리스트을 초기화하는 방법에 대해서 다양한 방법들이 있는 듯 한데, 3번 문제는 다 풀어놓고 빈 배열을 간파해서 계속 out of range 에러가 났었다.&lt;/p&gt;

&lt;p&gt;파이썬 정렬 시 sort함수는 원형 데이터를 바로 정렬하고, sorted함수는 원형을 보존하고 정렬 결과를 반환한다.  sorted함수에는 정렬 기준을 따로 줄 수 있는데, 람다식을 이용해서 1순위 기준, 2순위 기준을 한꺼번에 줄 수 있다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;sorted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sorted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;strings&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;strings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이렇게 쓴 식을&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;answer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sorted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;strings&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이렇게 다중 조건을 줄 수 있다.&lt;/p&gt;

&lt;p&gt;그리고 파이썬의 리스트 초기화에 대해서 알아봤는데 내가 쓴 방법은 이거였다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;그리고 list는 동적할당인데, numpy array는 정적할당이다! 조심할 것.&lt;/p&gt;</content><author><name>Hyemi Jeong</name></author><summary type="html">파이썬을 해봅시다 오전 기초 과정에서는 파이썬 문법 심화, 오후 풀잎 시간에는 파이썬 공부 및 코테 풀어보기(프로그래머스)를 진행했다. 오늘 주목한 것 : immutable vs mutable 그래서 코드를 직접 짜봐서 테스트를 해봤다. print(&quot;make 2 tuple variable&quot;) a = tuple([1,2,3]) b = a print(id(a), id(b)) print(&quot;assign new tuple at the other tuple&quot;) b = a + (4,) print(a) print(b) print(id(a), id(b)) print(&quot;Is it possible to reassign new tuple to original variable?&quot;) a = a + (4,) print(a) print(id(a)) try : print(&quot;How about change the element of tuple&quot;) a[3] = 1 except : print(&quot;this expression is invaild&quot;) make 2 tuple variable 139912658793168 139912658793168 assign new tuple at the other tuple (1, 2, 3) (1, 2, 3, 4) 139912658793168 139912658652336 Is it possible to reassign new tuple to original variable? (1, 2, 3, 4) 139912658652912 How about change the element of tuple this expression is invaild tuple은 ‘변경이 불가능한’ 자료구조다. 실제로 index로 원소에 접근해서 tuple을 조작하려고 할 때, 에러가 난다. 하지만 tuple에 새 원소를 더해서 다른 변수에 할당하는 것은 가능하다. 이로써 파이썬에서의 ‘변수’는 메모리 공간을 가리키는 역할을 하고, 이 메모리 공간에 변수를 사용해서 접근한 후 그 공간을 변경가능한지 여부가 immutable과 mutable을 나눈다고 결론지었다. C++에서 리스트의 경우 각 리스트는 포인터의 연속이고, 각 포인터가 가리키는 메모리 공간은 불연속성을 띌 수도 있다고 배웠는데 파이썬의 리스트의 원소들은 메모리를 가리키는 포인터의 연속이고, 이 포인터를 통해 메모리에 접근해서 메모리가 담고 있는 값을 변경가능하다. 참고 : https://bearwoong.tistory.com/85 그리고 풀어본 문제는 이것! https://programmers.co.kr/learn/courses/30/lessons/12915 https://programmers.co.kr/learn/courses/30/lessons/12916 https://programmers.co.kr/learn/courses/30/lessons/64061 파이썬 람다식에 대한 공부가 더 필요할 듯 하다. 람다식을 쓰면 정렬을 할 때 기준을 내 마음대로 할 수 있다. 그리고 파이썬 빈 리스트을 초기화하는 방법에 대해서 다양한 방법들이 있는 듯 한데, 3번 문제는 다 풀어놓고 빈 배열을 간파해서 계속 out of range 에러가 났었다. 파이썬 정렬 시 sort함수는 원형 데이터를 바로 정렬하고, sorted함수는 원형을 보존하고 정렬 결과를 반환한다. sorted함수에는 정렬 기준을 따로 줄 수 있는데, 람다식을 이용해서 1순위 기준, 2순위 기준을 한꺼번에 줄 수 있다. sorted(sorted(strings), key = lambda strings : strings[n]) 이렇게 쓴 식을 answer = sorted(strings, key = lambda strings : (strings[n] ,strings)) 이렇게 다중 조건을 줄 수 있다. 그리고 파이썬의 리스트 초기화에 대해서 알아봤는데 내가 쓴 방법은 이거였다. list = [0 for i in range(n)] 그리고 list는 동적할당인데, numpy array는 정적할당이다! 조심할 것.</summary></entry><entry><title type="html">[aiffel] Day 20</title><link href="/2021/01/25/AIFFEL-DAY-20.html" rel="alternate" type="text/html" title="[aiffel] Day 20" /><published>2021-01-25T00:00:00+09:00</published><updated>2021-01-25T00:00:00+09:00</updated><id>/2021/01/25/%5BAIFFEL%5D-DAY-20</id><content type="html" xml:base="/2021/01/25/AIFFEL-DAY-20.html">&lt;h1 id=&quot;cs231n-05&quot;&gt;CS231n-05&lt;/h1&gt;

&lt;p&gt;포스팅이 매우 격조했습니다. 한동안 컨디션이 매우 안좋았기 때문이죠. 주말동안 푹 쉰 건 아니지만 잘 먹어서 좀 나아진 듯 합니다. 매일 매일 기록하자! 가 모토인데 이제는 매일 매일 잠 잘 자고, 잘 따라가고, 적극적인 자세로 임하는 것이 목표가 되었습니다. 여러분, 건강하세요.&lt;/p&gt;

&lt;p&gt;오늘의 그림 출처는 여기 : https://cding.tistory.com/5&lt;/p&gt;

&lt;h4 id=&quot;convnets의-원리&quot;&gt;ConvNets의 원리&lt;/h4&gt;

&lt;p&gt;4강을 들은 것을 정리를 안했는데, 4강 맨 끝에서 Fully Connected Layer에 대한 개념이 나온다. 그래서 잠깐 4강 끝에서 나왔던 Fully Connected Function에 대한 개념을 짚고 넘어가려고 한다.&lt;/p&gt;

&lt;h4 id=&quot;fully-connected-function&quot;&gt;Fully Connected Function&lt;/h4&gt;

&lt;p&gt;layer에 대한 input을 벡터 x라고 했을때 선형 함수의 형태로 score function을 구한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/9955C8425B5741051C&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;레이어 하나를 거치는 것은 W1을 곱하는 것이고, W2를 곱하게 되면 레이어 하나를 더 거치게 되는 것이다. 이 때, 다음 레이어로 가기 전에 그 전의 레이어의 결과물을 정제한다. 위의 그림에서 max 함수가 이런 역할을 하게 되며, 이를 &lt;strong&gt;activation function&lt;/strong&gt;이라고 한다. activation function은 비선형 함수를 쓰게 되며,  비선형 함수의 종류는 다음과 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/99EB714D5B57410D16&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;왜 비선형 함수를 쓰게 되냐면, 만약 이 때도 선형함수를 쓰게 되면 중간 결과값에 대해 가중치를 제대로 줄 수 없기 때문이다. 이 전에 인간의 신경망에 대한 설명이 나오는데, 차라리 역치 개념을 써놨더라면 ‘아. 신경망과 인공 지능망은 이런 공통점이 있구나!’ 라고 이해했을 걸…&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;하나의 레이어는 여러 개의 노드로 이루어져 있다. 레이어에 속한 하나의 노드에서는 다음과 같은 행동이 발생한다.  (이전 노드의 결과값) * (노드와 노드 간의 가중치)의 결과값 들을 받아들여 모두 더한다.(Wx, linear) 그리고 이 값에 activation함수를 적용해서 다음 노드로 전할 값을 결정한다.(non-linear)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;해당 구조에서 Layer를 여러개 두면 어떻게 될까? 다음과 같은 결과가 나오겠지.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(output) = Wn * max(Wn-1 * max(Wn-2...W2 * max(W1 * x, 0),0),0) (여기서 output layer는 마지막 layer의 노드수 * 1의 크기를 가지는 Wn을 통해 도출된다.)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;이렇게 되면 해당 layer에 있는 모든 노드들은 다음 노드에 관여하게 되는 구조가 나온다. 그래서 Fully Connected Layer라고 부른다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/99DC70435B57410D02&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 다시 돌아가서, 5강의 처음은 Fully Connected Layer(&lt;strong&gt;Dense layer&lt;/strong&gt;)에 대한 아이디어로부터 출발하게 된다.&lt;/p&gt;

&lt;h4 id=&quot;convolutional-neural-networks의-시작&quot;&gt;Convolutional Neural Networks의 시작&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/991CF9415B61925C1A&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;행렬 곱은 (3072 x 1) * (10 x 3072) = (1 * 10) 이 나오게 된다. 이전 시간에 배웠던 Fully Connected Network의 원리를 생각해 보면 모든 input값은 다음 노드로 전달되니까 3072 x 1의 형태로 만든 것이고, hidden layer는 하나, class는 10개라서 output layer 모습이 1 * 10이 된다.&lt;/p&gt;

&lt;p&gt;그렇다면 Convolutional Neural Network는 Fully Connected Layer에서 어떻게 나아갔을까?&lt;/p&gt;

&lt;p&gt;데이터의 특성을 잘 반영하기 위해서 &lt;strong&gt;filter&lt;/strong&gt;의 개념을 도입했다.&lt;/p&gt;

&lt;p&gt;경험이 쌓이면 도움이 된다더니, 예전에 막내 국어 문제 풀어줄 때 나왔던 지문이 도움이 될 줄이야.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/9989CC495D342D2D2A&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;해당 문제는 2018년도 고2 모의평가 3월에 나왔던 지문인데, 풀어주면서도 ‘이걸 애들이 시간내에 풀 수 있을 거라고?’ 하며 연신 고개를 주억거렸던 기억이 난다. 이 그림은 필터를 통해 어떻게 이미지의 특성을 잡아내는지 그 예시를 들었다. 이 다음에 이 이미지를 보고 알맞게 추론한 것을 고르시오. 라는 문제와 함께 그림이 함께 나오는 예시가 나와서 명확히 기억한다.&lt;/p&gt;

&lt;p&gt;필터는 이미지의 좌상단부터 훑는다. 그리고 필터의 모든 요소를 가지고 내적을 하며, 하나의 값을 얻게 된다. 이 과정이 위 그림의 첫번째 줄에 해당한다. 이 연산의 결과를 activation map에 저장하게 된다.&lt;/p&gt;

&lt;p&gt;32x32x3을 5x5x3의 필터와 활성화 함수로 훑었을 때, 28x28x3의 activation map이 나온다.(n칸씩 뗄 지는 파라미터로 결정 =&amp;gt; stride라고 한다.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/990AEF335B61926033&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;정리하면 하나의 CNN은 filtering과 activation을 거친다. 그리고 하나의 activation map은 input data의 한 특성을 나타낸다. 바로 이렇게. 이 과정들을 반복하면  low-level features -&amp;gt; mid-level features -&amp;gt; high-level features 순으로 추출할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/9987664E5B61D3FD07&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그래서 전체적인 Convolutional Neural Network는 다음과 같이 생겼다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/99546A3F5B61926206&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그런데 아까전의 예시에서는 한칸씩 필터를 밀었다. 필터를 한칸씩 말고 n칸씩 밀 수 있지 않을까? 이 때 어정쩡하게 남는 부분은 잘리게 되지 않을까? 그리고 input 데이터의 가장자리는 중앙보다 반영되는 횟수가 적지 않을까? 또 n칸씩 확 확 밀면 Layer 몇 개만 거쳐도 중간 값의 차원이 확 줄지 않을까?&lt;/p&gt;

&lt;p&gt;그래서 &lt;strong&gt;Padding&lt;/strong&gt;의 개념이 나오게 된다. Padding은 원본 데이터의 가장자리에 0으로 채운 행과 열을 더함으로써 위의 두가지 문제를 해결하게 된다.&lt;/p&gt;

&lt;p&gt;테두리에 padding을 준다 : (N-F)/(stride+1). padding은 0으로 채운다.&lt;/p&gt;

&lt;p&gt;그리고 중간에 이해 안가는 부분이 있어서 데려와봤다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/99CD73375B61D6EF18&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;필터를 언급할 때 input 데이터의 depth에 대해서는 생략하고 얘기하더라. 적어도 해당 강의에서는. 그래서 조원들이랑 공부할 때 더더욱 헷갈렸다. 그림을 해석하면 다음과 같다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1x1 conv with 32 filters : 1x1x64의 filter를 사용하며, 이를 32번 반복한다.&lt;/li&gt;
  &lt;li&gt;결과값 56 x56 x 32 : 1x1x64를 한칸씩 밀어서 필터를 적용하고, 이에 activation function을 적용해서 만든 activation map의 크기는 56x56x1인데 이 과정을 32번 반복했으니까 depth는 32.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/9957CE405B61D6F10C&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;해당 그림을 보니까 이전의 그림이 더 명확하게 이해되는데, 5개의 다른 필터를 사용해서 이 결과를 쌓으면 depth가 늘어난다. 그래. 필터를 다른 것을 쓴다고 얘기를 했어야지. 이 때 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;필터 = 신경&lt;/code&gt; 이렇게 치환해서 생각해보자.&lt;/p&gt;

&lt;h4 id=&quot;pooling&quot;&gt;Pooling&lt;/h4&gt;

&lt;p&gt;하지만 이 사이즈를 유지한 채로 계속해서 연산하게 된다면, 이는 성능에 영향을 주게 된다. 그래서 Pooling을 통해 Downsampling을 하게 된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/99C286405B61D6F325&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;해당 그림은 Pooling기법 중 Max를 이용하는 Max Pooling에 대해서 설명하고 있다. 간단하다. 해당 영역에서 max값만 뽑아서 다음 값으로 넘긴다. 마치 그림의 해상도를 n분의 1로 줄일 때 처럼 말이다.&lt;/p&gt;

&lt;h4 id=&quot;fully-connected-layer-in-cnn&quot;&gt;Fully Connected Layer in CNN&lt;/h4&gt;

&lt;p&gt;해당 레이어는 마지막에 쓰인다. class를 뽑아내야 하기 때문이다. 우리가 앞에서 Fully Connected Layer는 이런 것이라고 정의했기 때문에, 작동 원리를 생각해서 마지막에 이를 적용하게 되면 우리는 1x(클래스의 수) 크기를 가진 벡터를 얻을 수 있음을 알 수 있다.&lt;/p&gt;

&lt;h4 id=&quot;오늘의-후기&quot;&gt;오늘의 후기&lt;/h4&gt;

&lt;p&gt;아…진짜…어후…일단 Fully Connected Layer라는 용어를 유심히 보지 못하고 지나간 것이 화근이었다. 내가 기존에 알고 있던 기초적인 신경망 구조를 Fully Connected Layer라고 부르는 것을 몰랐다.&lt;/p&gt;

&lt;p&gt;또 구체적인 예시를 들어야 할 때에는 정작 뭉뚱그려서 설명하는 강의의 방식, 표기법을 통일하지 않는 교재, 심지어 누락까지 되어 있어 이해를 더더욱 어렵게 만든다.&lt;/p&gt;

&lt;p&gt;이제 확실히 알겠으니까 다음 강의를 기다려 본다. 이상 저번 강의부터 헷갈린 사람의 후기.&lt;/p&gt;</content><author><name>Hyemi Jeong</name></author><summary type="html">CS231n-05 포스팅이 매우 격조했습니다. 한동안 컨디션이 매우 안좋았기 때문이죠. 주말동안 푹 쉰 건 아니지만 잘 먹어서 좀 나아진 듯 합니다. 매일 매일 기록하자! 가 모토인데 이제는 매일 매일 잠 잘 자고, 잘 따라가고, 적극적인 자세로 임하는 것이 목표가 되었습니다. 여러분, 건강하세요. 오늘의 그림 출처는 여기 : https://cding.tistory.com/5 ConvNets의 원리 4강을 들은 것을 정리를 안했는데, 4강 맨 끝에서 Fully Connected Layer에 대한 개념이 나온다. 그래서 잠깐 4강 끝에서 나왔던 Fully Connected Function에 대한 개념을 짚고 넘어가려고 한다. Fully Connected Function layer에 대한 input을 벡터 x라고 했을때 선형 함수의 형태로 score function을 구한다. 레이어 하나를 거치는 것은 W1을 곱하는 것이고, W2를 곱하게 되면 레이어 하나를 더 거치게 되는 것이다. 이 때, 다음 레이어로 가기 전에 그 전의 레이어의 결과물을 정제한다. 위의 그림에서 max 함수가 이런 역할을 하게 되며, 이를 activation function이라고 한다. activation function은 비선형 함수를 쓰게 되며, 비선형 함수의 종류는 다음과 같다. 왜 비선형 함수를 쓰게 되냐면, 만약 이 때도 선형함수를 쓰게 되면 중간 결과값에 대해 가중치를 제대로 줄 수 없기 때문이다. 이 전에 인간의 신경망에 대한 설명이 나오는데, 차라리 역치 개념을 써놨더라면 ‘아. 신경망과 인공 지능망은 이런 공통점이 있구나!’ 라고 이해했을 걸… 하나의 레이어는 여러 개의 노드로 이루어져 있다. 레이어에 속한 하나의 노드에서는 다음과 같은 행동이 발생한다. (이전 노드의 결과값) * (노드와 노드 간의 가중치)의 결과값 들을 받아들여 모두 더한다.(Wx, linear) 그리고 이 값에 activation함수를 적용해서 다음 노드로 전할 값을 결정한다.(non-linear) 해당 구조에서 Layer를 여러개 두면 어떻게 될까? 다음과 같은 결과가 나오겠지. (output) = Wn * max(Wn-1 * max(Wn-2...W2 * max(W1 * x, 0),0),0) (여기서 output layer는 마지막 layer의 노드수 * 1의 크기를 가지는 Wn을 통해 도출된다.) 이렇게 되면 해당 layer에 있는 모든 노드들은 다음 노드에 관여하게 되는 구조가 나온다. 그래서 Fully Connected Layer라고 부른다. 이제 다시 돌아가서, 5강의 처음은 Fully Connected Layer(Dense layer)에 대한 아이디어로부터 출발하게 된다. Convolutional Neural Networks의 시작 행렬 곱은 (3072 x 1) * (10 x 3072) = (1 * 10) 이 나오게 된다. 이전 시간에 배웠던 Fully Connected Network의 원리를 생각해 보면 모든 input값은 다음 노드로 전달되니까 3072 x 1의 형태로 만든 것이고, hidden layer는 하나, class는 10개라서 output layer 모습이 1 * 10이 된다. 그렇다면 Convolutional Neural Network는 Fully Connected Layer에서 어떻게 나아갔을까? 데이터의 특성을 잘 반영하기 위해서 filter의 개념을 도입했다. 경험이 쌓이면 도움이 된다더니, 예전에 막내 국어 문제 풀어줄 때 나왔던 지문이 도움이 될 줄이야. 해당 문제는 2018년도 고2 모의평가 3월에 나왔던 지문인데, 풀어주면서도 ‘이걸 애들이 시간내에 풀 수 있을 거라고?’ 하며 연신 고개를 주억거렸던 기억이 난다. 이 그림은 필터를 통해 어떻게 이미지의 특성을 잡아내는지 그 예시를 들었다. 이 다음에 이 이미지를 보고 알맞게 추론한 것을 고르시오. 라는 문제와 함께 그림이 함께 나오는 예시가 나와서 명확히 기억한다. 필터는 이미지의 좌상단부터 훑는다. 그리고 필터의 모든 요소를 가지고 내적을 하며, 하나의 값을 얻게 된다. 이 과정이 위 그림의 첫번째 줄에 해당한다. 이 연산의 결과를 activation map에 저장하게 된다. 32x32x3을 5x5x3의 필터와 활성화 함수로 훑었을 때, 28x28x3의 activation map이 나온다.(n칸씩 뗄 지는 파라미터로 결정 =&amp;gt; stride라고 한다.) 정리하면 하나의 CNN은 filtering과 activation을 거친다. 그리고 하나의 activation map은 input data의 한 특성을 나타낸다. 바로 이렇게. 이 과정들을 반복하면 low-level features -&amp;gt; mid-level features -&amp;gt; high-level features 순으로 추출할 수 있다. 그래서 전체적인 Convolutional Neural Network는 다음과 같이 생겼다. 그런데 아까전의 예시에서는 한칸씩 필터를 밀었다. 필터를 한칸씩 말고 n칸씩 밀 수 있지 않을까? 이 때 어정쩡하게 남는 부분은 잘리게 되지 않을까? 그리고 input 데이터의 가장자리는 중앙보다 반영되는 횟수가 적지 않을까? 또 n칸씩 확 확 밀면 Layer 몇 개만 거쳐도 중간 값의 차원이 확 줄지 않을까? 그래서 Padding의 개념이 나오게 된다. Padding은 원본 데이터의 가장자리에 0으로 채운 행과 열을 더함으로써 위의 두가지 문제를 해결하게 된다. 테두리에 padding을 준다 : (N-F)/(stride+1). padding은 0으로 채운다. 그리고 중간에 이해 안가는 부분이 있어서 데려와봤다. 필터를 언급할 때 input 데이터의 depth에 대해서는 생략하고 얘기하더라. 적어도 해당 강의에서는. 그래서 조원들이랑 공부할 때 더더욱 헷갈렸다. 그림을 해석하면 다음과 같다. 1x1 conv with 32 filters : 1x1x64의 filter를 사용하며, 이를 32번 반복한다. 결과값 56 x56 x 32 : 1x1x64를 한칸씩 밀어서 필터를 적용하고, 이에 activation function을 적용해서 만든 activation map의 크기는 56x56x1인데 이 과정을 32번 반복했으니까 depth는 32. 해당 그림을 보니까 이전의 그림이 더 명확하게 이해되는데, 5개의 다른 필터를 사용해서 이 결과를 쌓으면 depth가 늘어난다. 그래. 필터를 다른 것을 쓴다고 얘기를 했어야지. 이 때 필터 = 신경 이렇게 치환해서 생각해보자. Pooling 하지만 이 사이즈를 유지한 채로 계속해서 연산하게 된다면, 이는 성능에 영향을 주게 된다. 그래서 Pooling을 통해 Downsampling을 하게 된다. 해당 그림은 Pooling기법 중 Max를 이용하는 Max Pooling에 대해서 설명하고 있다. 간단하다. 해당 영역에서 max값만 뽑아서 다음 값으로 넘긴다. 마치 그림의 해상도를 n분의 1로 줄일 때 처럼 말이다. Fully Connected Layer in CNN 해당 레이어는 마지막에 쓰인다. class를 뽑아내야 하기 때문이다. 우리가 앞에서 Fully Connected Layer는 이런 것이라고 정의했기 때문에, 작동 원리를 생각해서 마지막에 이를 적용하게 되면 우리는 1x(클래스의 수) 크기를 가진 벡터를 얻을 수 있음을 알 수 있다. 오늘의 후기 아…진짜…어후…일단 Fully Connected Layer라는 용어를 유심히 보지 못하고 지나간 것이 화근이었다. 내가 기존에 알고 있던 기초적인 신경망 구조를 Fully Connected Layer라고 부르는 것을 몰랐다. 또 구체적인 예시를 들어야 할 때에는 정작 뭉뚱그려서 설명하는 강의의 방식, 표기법을 통일하지 않는 교재, 심지어 누락까지 되어 있어 이해를 더더욱 어렵게 만든다. 이제 확실히 알겠으니까 다음 강의를 기다려 본다. 이상 저번 강의부터 헷갈린 사람의 후기.</summary></entry><entry><title type="html">[aiffel] Day 11</title><link href="/2021/01/12/AIFFEL-DAY-11.html" rel="alternate" type="text/html" title="[aiffel] Day 11" /><published>2021-01-12T00:00:00+09:00</published><updated>2021-01-12T00:00:00+09:00</updated><id>/2021/01/12/%5BAIFFEL%5D-DAY-11</id><content type="html" xml:base="/2021/01/12/AIFFEL-DAY-11.html">&lt;h1 id=&quot;20210112&quot;&gt;20210112&lt;/h1&gt;

&lt;h3 id=&quot;부제--스티커-사진-프로그램-만들기&quot;&gt;부제 : 스티커 사진 프로그램 만들기&lt;/h3&gt;

&lt;p&gt;오늘 노드에서는 얼굴 인식의 방법과 얼굴 인식 결과를 간단하게 응용해서 얼굴에 스티커를 붙이는 프로그램을 작성했다.&lt;/p&gt;

&lt;p&gt;OpenCV의 기능들을 사용하는 게 중요했는데, OpenCV가 처리하는 이미지 데이터의 원하는 부분에 제대로 접근하려면 좌표 개념이 매우 중요하다. OpenCV는 행렬의 방식으로 데이터를 표현하고 있기 때문에, y축을 먼저 기술하고 x축을 기술하게 되며, &lt;strong&gt;좌상단이 원점&lt;/strong&gt;이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/2610B8475436287821&quot; alt=&quot;카이제곱 :: '컴퓨터비전/영상처리' 카테고리의 글 목록 (5 Page)&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 좌표가 오늘의 핵심 키포인트. tmi로 계속 3차원 데이터를 다루다가 2차원 데이터를 다루게 되니 조금 어색했다. 3차원 좌표계는 또 달라서…&lt;/p&gt;

&lt;p&gt;그리고 HOG 모델을 통해 얼굴의 특이점(landmark)을 추출해 내어 사진에서 얼굴을 찾을 수 있었다. 간단히 설명하면 이미지를 일정 구역들로 나누고 , 그 안에서 Gradient를 찾아낸다. 이 Gradient에서 얼굴 패턴으로 보이는 것을 찾아낸다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/800/1*HtgQZ4guaIo8wflbsR1MLw.png&quot; alt=&quot;Image for post&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://blog.kakaocdn.net/dn/c8fGmV/btqGQcZBwNz/JzcH66lqvnxL7WDd3xo7EK/img.png&quot; alt=&quot;얼굴인식 출입통제 프로그램을 만들어보세요.(유튜브 소개)&quot; /&gt;&lt;/p&gt;

&lt;p&gt;우리가 쓰는 카메라 어플에서 얼굴을 인식하는 방법 중 하나가 이 랜드마크를 사용하는 방법이라고 한다.&lt;/p&gt;

&lt;p&gt;그리고는 스티커 사이즈를 얼굴 이미지에 따라 크기를 조정하고, 이미지 가공을 하게 된다. 스티커를 놓을 자리를 랜드마크의 좌표에 따라서 결정하고, 비율을 랜드마크를 이용해서 조절하게 된다.&lt;/p&gt;

&lt;p&gt;이미지는 numpy의 array 타입으로 다룬다. 그리고 OpenCV의 기능을 활용하면 이미지 간의 연산이 가능하다. 이 둘을 적절히 조합하면 스티커를 이미지 위에 적절히 올릴 수 있다.&lt;/p&gt;

&lt;h4 id=&quot;후기&quot;&gt;후기&lt;/h4&gt;

&lt;p&gt;지금은 단순히 스티커가 놓일 위치를 해당하는 영역의 랜드마크들을 둘러싸는 bounding box로 구하고 있는데, 좀 더 정밀하게 하려면 랜드마크를 정확히 둘러싸는 minimum bounding box를 구하고,  affine transformation을 통해 얼굴의 각도가 틀어진다거나 하는 그런 경우에 대비할 수 있을 것이다.&lt;/p&gt;

&lt;p&gt;노드 이제 정리하고 회고만 넣으면 끝!&lt;/p&gt;

&lt;p&gt;오늘 노드 진행한 코드는 후에 보완할 예정입니다.&lt;/p&gt;</content><author><name>Hyemi Jeong</name></author><summary type="html">20210112 부제 : 스티커 사진 프로그램 만들기 오늘 노드에서는 얼굴 인식의 방법과 얼굴 인식 결과를 간단하게 응용해서 얼굴에 스티커를 붙이는 프로그램을 작성했다. OpenCV의 기능들을 사용하는 게 중요했는데, OpenCV가 처리하는 이미지 데이터의 원하는 부분에 제대로 접근하려면 좌표 개념이 매우 중요하다. OpenCV는 행렬의 방식으로 데이터를 표현하고 있기 때문에, y축을 먼저 기술하고 x축을 기술하게 되며, 좌상단이 원점이다. 이 좌표가 오늘의 핵심 키포인트. tmi로 계속 3차원 데이터를 다루다가 2차원 데이터를 다루게 되니 조금 어색했다. 3차원 좌표계는 또 달라서… 그리고 HOG 모델을 통해 얼굴의 특이점(landmark)을 추출해 내어 사진에서 얼굴을 찾을 수 있었다. 간단히 설명하면 이미지를 일정 구역들로 나누고 , 그 안에서 Gradient를 찾아낸다. 이 Gradient에서 얼굴 패턴으로 보이는 것을 찾아낸다. 우리가 쓰는 카메라 어플에서 얼굴을 인식하는 방법 중 하나가 이 랜드마크를 사용하는 방법이라고 한다. 그리고는 스티커 사이즈를 얼굴 이미지에 따라 크기를 조정하고, 이미지 가공을 하게 된다. 스티커를 놓을 자리를 랜드마크의 좌표에 따라서 결정하고, 비율을 랜드마크를 이용해서 조절하게 된다. 이미지는 numpy의 array 타입으로 다룬다. 그리고 OpenCV의 기능을 활용하면 이미지 간의 연산이 가능하다. 이 둘을 적절히 조합하면 스티커를 이미지 위에 적절히 올릴 수 있다. 후기 지금은 단순히 스티커가 놓일 위치를 해당하는 영역의 랜드마크들을 둘러싸는 bounding box로 구하고 있는데, 좀 더 정밀하게 하려면 랜드마크를 정확히 둘러싸는 minimum bounding box를 구하고, affine transformation을 통해 얼굴의 각도가 틀어진다거나 하는 그런 경우에 대비할 수 있을 것이다. 노드 이제 정리하고 회고만 넣으면 끝! 오늘 노드 진행한 코드는 후에 보완할 예정입니다.</summary></entry><entry><title type="html">[aiffel] Day 10</title><link href="/2021/01/11/AIFFEL-DAY-10.html" rel="alternate" type="text/html" title="[aiffel] Day 10" /><published>2021-01-11T00:00:00+09:00</published><updated>2021-01-11T00:00:00+09:00</updated><id>/2021/01/11/%5BAIFFEL%5D-DAY-10</id><content type="html" xml:base="/2021/01/11/AIFFEL-DAY-10.html">&lt;h1 id=&quot;20210111&quot;&gt;20210111&lt;/h1&gt;

&lt;p&gt;오전 : 머신러닝이란? 딥러닝이란? 에서 출발하는 질문들을 점차 심층적으로 내려가는 질문들의 사례를 보았다.&lt;/p&gt;

&lt;h2 id=&quot;cs231n-03-02&quot;&gt;CS231n-03-02&lt;/h2&gt;

&lt;h3 id=&quot;optimization&quot;&gt;Optimization&lt;/h3&gt;

&lt;p&gt;계곡에 있다고 생각하고, 우리는 계곡의 가장 낮은 밑바닥을 찾아내는 것이 목적이다. 지금 우리가 있는 곳의 높이가 loss, 그리고 밑바닥을 찾아가는 여정이 optimization과정이 된다. 풍경은 파라미터 W를 의미한다. W는 loss에 따라서 달라지니까.&lt;/p&gt;

&lt;p&gt;loss를 줄이는 방법은 Random search 방법이 있다.  (정확도 15.5퍼센트 정도 나온다고…)&lt;/p&gt;

&lt;p&gt;두 번째 방법으로 경사의 기울기에 따라서 길을 찾아볼까?(NN, Linear regression)&lt;/p&gt;

&lt;p&gt;특정 지점에서 미분을 해서 기울기를 구해보자. 그렇다면 각 변수에 대한 편미분함수의 조합이 되겠지. 유닛벡터와 gradient의 조합이 해당 지점에서의 기울기를 알려줄 것이다.&lt;/p&gt;

&lt;h5 id=&quot;수치적인-방법&quot;&gt;수치적인 방법&lt;/h5&gt;

&lt;p&gt;현재 W-&amp;gt;W+h(W의 각 dimension에 대해서 이를 반복한다.)-&amp;gt;gradient인 dW 확인 =&amp;gt; 이거 진짜 느리고 각 dimension에 대해서 함수 계산을 다시 계산해야 해서 시간 너무 걸린다. &lt;strong&gt;다만 디버깅을 위해서 쓰일 수 있다.&lt;/strong&gt; 유닛 테스트 처럼. 이때는 파라미터를 줄여서 실시한다. (gradient check)&lt;/p&gt;

&lt;p&gt;정리하면 근접적이고, 느리고, 적기에는 편하다.&lt;/p&gt;

&lt;h5 id=&quot;분석적인-방법&quot;&gt;분석적인 방법&lt;/h5&gt;

&lt;p&gt;시간을 축소하기 위해 걸리는 방식 : loss function을 적어놓고 미분함수를 구한 후 이의 극한을 구한다.&lt;/p&gt;

&lt;p&gt;정리하면 빠르고, 정확하다. 근데 실수하기 쉬움.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;![Tuning the learning rate in Gradient Descent&lt;/td&gt;
      &lt;td&gt;Datumbox](https://blog.datumbox.com/wp-content/uploads/2013/10/gradient-descent.png)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h5 id=&quot;gradient-descent&quot;&gt;Gradient descent&lt;/h5&gt;

&lt;p&gt;W를 임의의 값으로 초기화한다. 그리고 gradient(접평면에 수직인 백터)가 -로 나오는 방향으로 weight를 조정해줄 것이다. 스텝 사이즈를 통해서 학습을 했을 때, 그 학습의 속도를 Learning Rate라고 한다. Step size는 하이퍼파라미터의 하나이다. 학습의 방향에 중요한 역할을 한다.&lt;/p&gt;

&lt;p&gt;효과적인 Gradient descent를 위해서 이전 Gradient descent 결과에 피드백을 주는 방법이 있다.&lt;/p&gt;

&lt;p&gt;Q. step size vs learning rate vs batch size&lt;/p&gt;

&lt;p&gt;step size : 보폭의 크기&lt;/p&gt;

&lt;p&gt;learning rate : 보폭의 크기 내에서도 한번에 학습하는 정도…?&lt;/p&gt;

&lt;p&gt;Gradient descent에 대해서 더 참고하려면 여기를~&lt;/p&gt;

&lt;p&gt;https://seamless.tistory.com/38&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/993D383359D86C280D&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;stochastic-gradient-descentsgd&quot;&gt;Stochastic Gradient Descent(SGD)&lt;/h5&gt;

&lt;p&gt;만약 N이 엄청나게 크면 어떡하지? 실제 gradient를 계산하려면 N개의 데이터를 돌면서 다 반영해야 할 것이다. 그래서 전체 데이터 셋의 gradient와 loss를 다 계산하기 보다는 Minibatch를 쓴다.(2의 승수를 주로 쓴다.) 그러니까 전체 집단 보다 데이터 셋에서 임의의 minibatch를 구해서 거기서 loss와 gradient를 구한다.&lt;/p&gt;

&lt;p&gt;cf) Optimizer 계보 참조해서 넣기&lt;/p&gt;

&lt;h5 id=&quot;image-features-학습하기&quot;&gt;Image Features 학습하기&lt;/h5&gt;

&lt;p&gt;이전에는 여러 속성들을 벡터로 엮어서 Linear classification에 입력했다. Feature들을 transform해서 선형 분류가 되도록 바꾼다거나. 예를 들자면 컬러 히스토그램이 있다. 이미지에서 Hue값만 뽑아서 색깔 테이블에 매칭해 보는 것이다.&lt;/p&gt;

&lt;p&gt;Histogram Oriented Gradient : local gradient을 통해서 각 local별로 어떤 edge가 존재하는지 알아볼 수 있다. 그리고 해당 local에서 어떤 vector가 많이 등장하는지 양동이에 담는다. 예를 들어 자연어 처리에서 Bag of Words에서 문장의 여러 단어의 발생 빈도를 재서 양동이에 담는 경우를 떠올릴 수 있다. 이미지의 경우는 많은 이미지를 구해서, 이 이미지를 조각조각낸 후, 그 조각에서 
“virtual words”를 찾아낸다. 이를 모은 것을 “code book”이라고 할 때, 이 code book에 따라 이미지를 분류할 수 있을 것이다.&lt;/p&gt;

&lt;p&gt;10년 전만 해도 이미지 입력-&amp;gt;BOW/HOG-&amp;gt;특징 추출-&amp;gt;분류기에 입력. 한번 이미지의 속성이 추출되면, 이는 변하지 않는다. 분류기만 학습이 될 뿐.&lt;/p&gt;

&lt;p&gt;딥러닝은 이제 이 속성까지 스스로 추출해 낸다는 것!&lt;/p&gt;

&lt;h2 id=&quot;cs231-04-01&quot;&gt;CS231-04-01&lt;/h2&gt;

&lt;h3 id=&quot;backpropagation-and-neural-networks&quot;&gt;Backpropagation and Neural Networks&lt;/h3&gt;

&lt;p&gt;어떻게 분석적인 방법을 통해 gradient descent를 실행하는가?&lt;/p&gt;

&lt;p&gt;computational graph를 그려서 뉴럴 네트워크의 구조를 설명하더라.&lt;/p&gt;

&lt;h5 id=&quot;backpropagation&quot;&gt;Backpropagation&lt;/h5&gt;

&lt;p&gt;뒤에서부터 앞으로 반대방향으로 편미분을 한다.&lt;/p&gt;

&lt;p&gt;그렇다면 Backpropagation을 왜 하냐?(What is backpropagation really doing?) : 해당 네트워크가 제대로 학습하고 있는지를 알아보기 위해서 디버깅하는 과정이다. output으로 loss값을 구했다면, 이 다음 학습을 위해 weight를 새롭게 정비해야 한다. 따라서 역으로 거슬러 올라가면서 weight 값들을 변경하게 된다.&lt;/p&gt;

&lt;p&gt;local gradient (편미분을 통해서)&lt;/p&gt;

&lt;p&gt;해당 노드에서 local gradient를 구할려면 이전 노드 결과 x, y가 있다 치면 최종 loss function을 x,y로 편미분한 값들을 구한다. (chain rule을 통해 편미분 구함) output 값인 loss값이 상수이기 때문에 여기서 local gradient를 구할 수 있고, 이는 이전의 W에 -피드백을 하기 위해 쓰인다.(Gradient가 -여야 아래로 내려가는 방향의 피드백을 할 수 있기 때문에. 그래프를 생각해 보자.)&lt;/p&gt;

&lt;p&gt;결론 : 정방향으로 갈 때는 모델에 데이터가 지나가면서 최종 학습 결과를 내놓는다. 역방향으로 갈 때는 최종 학습 결과와 loss값을 이용해서 각 레이어 사이의 weight값을 조정한다. 왔다 갔다, 핑퐁핑퐁.&lt;/p&gt;

&lt;h3 id=&quot;후기&quot;&gt;후기&lt;/h3&gt;

&lt;p&gt;아침에 진짜 읽어볼 양이 많았다. 노드를 수행하기가 좀 버거웠고, 아침에 약한 타입임을 다시 한번 느꼈다. 이런. 오후 시간은 Backpropagation이 왜 이루어지는지 드디어 알아냈다! 학부시간에 주입식으로 들었던 시간들이 억울할 지경이다. 오늘도 이렇게 한발짝 나아간다.&lt;/p&gt;</content><author><name>Hyemi Jeong</name></author><summary type="html">20210111 오전 : 머신러닝이란? 딥러닝이란? 에서 출발하는 질문들을 점차 심층적으로 내려가는 질문들의 사례를 보았다. CS231n-03-02 Optimization 계곡에 있다고 생각하고, 우리는 계곡의 가장 낮은 밑바닥을 찾아내는 것이 목적이다. 지금 우리가 있는 곳의 높이가 loss, 그리고 밑바닥을 찾아가는 여정이 optimization과정이 된다. 풍경은 파라미터 W를 의미한다. W는 loss에 따라서 달라지니까. loss를 줄이는 방법은 Random search 방법이 있다. (정확도 15.5퍼센트 정도 나온다고…) 두 번째 방법으로 경사의 기울기에 따라서 길을 찾아볼까?(NN, Linear regression) 특정 지점에서 미분을 해서 기울기를 구해보자. 그렇다면 각 변수에 대한 편미분함수의 조합이 되겠지. 유닛벡터와 gradient의 조합이 해당 지점에서의 기울기를 알려줄 것이다. 수치적인 방법 현재 W-&amp;gt;W+h(W의 각 dimension에 대해서 이를 반복한다.)-&amp;gt;gradient인 dW 확인 =&amp;gt; 이거 진짜 느리고 각 dimension에 대해서 함수 계산을 다시 계산해야 해서 시간 너무 걸린다. 다만 디버깅을 위해서 쓰일 수 있다. 유닛 테스트 처럼. 이때는 파라미터를 줄여서 실시한다. (gradient check) 정리하면 근접적이고, 느리고, 적기에는 편하다. 분석적인 방법 시간을 축소하기 위해 걸리는 방식 : loss function을 적어놓고 미분함수를 구한 후 이의 극한을 구한다. 정리하면 빠르고, 정확하다. 근데 실수하기 쉬움. ![Tuning the learning rate in Gradient Descent Datumbox](https://blog.datumbox.com/wp-content/uploads/2013/10/gradient-descent.png) Gradient descent W를 임의의 값으로 초기화한다. 그리고 gradient(접평면에 수직인 백터)가 -로 나오는 방향으로 weight를 조정해줄 것이다. 스텝 사이즈를 통해서 학습을 했을 때, 그 학습의 속도를 Learning Rate라고 한다. Step size는 하이퍼파라미터의 하나이다. 학습의 방향에 중요한 역할을 한다. 효과적인 Gradient descent를 위해서 이전 Gradient descent 결과에 피드백을 주는 방법이 있다. Q. step size vs learning rate vs batch size step size : 보폭의 크기 learning rate : 보폭의 크기 내에서도 한번에 학습하는 정도…? Gradient descent에 대해서 더 참고하려면 여기를~ https://seamless.tistory.com/38 Stochastic Gradient Descent(SGD) 만약 N이 엄청나게 크면 어떡하지? 실제 gradient를 계산하려면 N개의 데이터를 돌면서 다 반영해야 할 것이다. 그래서 전체 데이터 셋의 gradient와 loss를 다 계산하기 보다는 Minibatch를 쓴다.(2의 승수를 주로 쓴다.) 그러니까 전체 집단 보다 데이터 셋에서 임의의 minibatch를 구해서 거기서 loss와 gradient를 구한다. cf) Optimizer 계보 참조해서 넣기 Image Features 학습하기 이전에는 여러 속성들을 벡터로 엮어서 Linear classification에 입력했다. Feature들을 transform해서 선형 분류가 되도록 바꾼다거나. 예를 들자면 컬러 히스토그램이 있다. 이미지에서 Hue값만 뽑아서 색깔 테이블에 매칭해 보는 것이다. Histogram Oriented Gradient : local gradient을 통해서 각 local별로 어떤 edge가 존재하는지 알아볼 수 있다. 그리고 해당 local에서 어떤 vector가 많이 등장하는지 양동이에 담는다. 예를 들어 자연어 처리에서 Bag of Words에서 문장의 여러 단어의 발생 빈도를 재서 양동이에 담는 경우를 떠올릴 수 있다. 이미지의 경우는 많은 이미지를 구해서, 이 이미지를 조각조각낸 후, 그 조각에서 “virtual words”를 찾아낸다. 이를 모은 것을 “code book”이라고 할 때, 이 code book에 따라 이미지를 분류할 수 있을 것이다. 10년 전만 해도 이미지 입력-&amp;gt;BOW/HOG-&amp;gt;특징 추출-&amp;gt;분류기에 입력. 한번 이미지의 속성이 추출되면, 이는 변하지 않는다. 분류기만 학습이 될 뿐. 딥러닝은 이제 이 속성까지 스스로 추출해 낸다는 것! CS231-04-01 Backpropagation and Neural Networks 어떻게 분석적인 방법을 통해 gradient descent를 실행하는가? computational graph를 그려서 뉴럴 네트워크의 구조를 설명하더라. Backpropagation 뒤에서부터 앞으로 반대방향으로 편미분을 한다. 그렇다면 Backpropagation을 왜 하냐?(What is backpropagation really doing?) : 해당 네트워크가 제대로 학습하고 있는지를 알아보기 위해서 디버깅하는 과정이다. output으로 loss값을 구했다면, 이 다음 학습을 위해 weight를 새롭게 정비해야 한다. 따라서 역으로 거슬러 올라가면서 weight 값들을 변경하게 된다. local gradient (편미분을 통해서) 해당 노드에서 local gradient를 구할려면 이전 노드 결과 x, y가 있다 치면 최종 loss function을 x,y로 편미분한 값들을 구한다. (chain rule을 통해 편미분 구함) output 값인 loss값이 상수이기 때문에 여기서 local gradient를 구할 수 있고, 이는 이전의 W에 -피드백을 하기 위해 쓰인다.(Gradient가 -여야 아래로 내려가는 방향의 피드백을 할 수 있기 때문에. 그래프를 생각해 보자.) 결론 : 정방향으로 갈 때는 모델에 데이터가 지나가면서 최종 학습 결과를 내놓는다. 역방향으로 갈 때는 최종 학습 결과와 loss값을 이용해서 각 레이어 사이의 weight값을 조정한다. 왔다 갔다, 핑퐁핑퐁. 후기 아침에 진짜 읽어볼 양이 많았다. 노드를 수행하기가 좀 버거웠고, 아침에 약한 타입임을 다시 한번 느꼈다. 이런. 오후 시간은 Backpropagation이 왜 이루어지는지 드디어 알아냈다! 학부시간에 주입식으로 들었던 시간들이 억울할 지경이다. 오늘도 이렇게 한발짝 나아간다.</summary></entry><entry><title type="html">[aiffel] Day 9</title><link href="/2021/01/08/AIFFEL-DAY-9.html" rel="alternate" type="text/html" title="[aiffel] Day 9" /><published>2021-01-08T00:00:00+09:00</published><updated>2021-01-08T00:00:00+09:00</updated><id>/2021/01/08/%5BAIFFEL%5D-DAY-9</id><content type="html" xml:base="/2021/01/08/AIFFEL-DAY-9.html"></content><author><name>Hyemi Jeong</name></author><summary type="html"></summary></entry></feed>