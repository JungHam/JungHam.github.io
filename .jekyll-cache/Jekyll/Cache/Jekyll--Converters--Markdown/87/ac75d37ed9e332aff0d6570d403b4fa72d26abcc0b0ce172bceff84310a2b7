I"	<h1 id="지시딥-준비">지시딥 준비</h1>

<h2 id="부제--밑바닥부터-시작하는-딥러닝-1권-5장">부제 : 밑바닥부터 시작하는 딥러닝 1권 5장</h2>

<h3 id="사전학습">사전학습</h3>

<p>일단 앞 부분을 다시 듣고 수식 전개를 해야할 것 같다. 이 부분이 어렵게 느껴지는건 당연하다고 하니 다행이다. 그리고 효율적인 전파와 역전파 계산을 위해 중간 결과값들을 캐시하는게 맞았다. 코드로 어떻게 구현하는 지가 궁금했는데 이렇게 하는거구나…</p>

<h3 id="역전파">역전파</h3>

<p>역전파가 하는 일은 연쇄 법칙의 원리와 같다</p>

<p>덧셈 노드의 경우는 미분하면 1이 나와서 1을 역전파로 전해진 값에 곱하는 거라 입력된 값을 그대로 다음 노드로 전하게 된다.</p>

<p>곱셈 노드의 경우 상류의 값에 순전파 때의 입력 신호들을 서로 바꾼 값을 곱해서 하류로 보낸다. 미분할때 다른 변수는 상수로 취급하니까.</p>

<p>#### ReLU</p>

<p>미분하면 입력값이 0보다 클때 1, 0 이하면 0이라서 0 이하로 신호가 들어오면 역전파 때는 더 이상 신호가 전파되지 않는다. (0을 보내버림). 전기 회로의 ‘스위치’ 에 비유할 수 있다고.</p>

<h4 id="sigmoid-계층">Sigmoid 계층</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
</code></pre></div></div>

<p>연쇄법칙에 따라서 정리하면 <code class="language-plaintext highlighter-rouge">델타y(1-y)</code>로 정리된다. 와…! 순전파의 출력(y)만으로 계산할 수 있겠군.</p>

<h4 id="affine">Affine</h4>

<p>신경망의 순전파 때 수행하는 행렬의 곱을 기하학에서는 affine 변환이라고 부른다.</p>

<p>해당 책의 코드에서는 X * W + B = O로 표현하고 있다. 앞의 계산 그래프들에서는 단일 실수값을 통과시켰는데 이제는 계산 그래프 간에 행렬이 지나가는 것이다.</p>

<ul>
  <li>더 알아볼 점 : 행렬이 끼여 있을 때의 미분</li>
</ul>
:ET