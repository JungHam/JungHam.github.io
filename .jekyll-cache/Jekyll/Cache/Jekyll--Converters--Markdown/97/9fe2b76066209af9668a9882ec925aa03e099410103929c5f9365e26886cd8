I"$<h1 id="cs231n-05">CS231n-05</h1>

<p>포스팅이 매우 격조했습니다. 한동안 컨디션이 매우 안좋았기 때문이죠. 주말동안 푹 쉰 건 아니지만 잘 먹어서 좀 나아진 듯 합니다. 매일 매일 기록하자! 가 모토인데 이제는 매일 매일 잠 잘 자고, 잘 따라가고, 적극적인 자세로 임하는 것이 목표가 되었습니다. 여러분, 건강하세요.</p>

<p>오늘의 그림 출처는 여기 : https://cding.tistory.com/5</p>

<h4 id="convnets의-원리">ConvNets의 원리</h4>

<p>4강을 들은 것을 정리를 안했는데, 4강 맨 끝에서 Fully Connected Layer에 대한 개념이 나온다. 그래서 잠깐 4강 끝에서 나왔던 Fully Connected Function에 대한 개념을 짚고 넘어가려고 한다.</p>

<h4 id="fully-connected-function">Fully Connected Function</h4>

<p>layer에 대한 input을 벡터 x라고 했을때 선형 함수의 형태로 score function을 구한다.</p>

<p><img src="https://t1.daumcdn.net/cfile/tistory/9955C8425B5741051C" alt="img" /></p>

<p>레이어 하나를 거치는 것은 W1을 곱하는 것이고, W2를 곱하게 되면 레이어 하나를 더 거치게 되는 것이다. 이 때, 다음 레이어로 가기 전에 그 전의 레이어의 결과물을 정제한다. 위의 그림에서 max 함수가 이런 역할을 하게 되며, 이를 <strong>activation function</strong>이라고 한다. activation function은 비선형 함수를 쓰게 되며,  비선형 함수의 종류는 다음과 같다.</p>

<p><img src="https://t1.daumcdn.net/cfile/tistory/99EB714D5B57410D16" alt="img" /></p>

<p>왜 비선형 함수를 쓰게 되냐면, 만약 이 때도 선형함수를 쓰게 되면 중간 결과값에 대해 가중치를 제대로 줄 수 없기 때문이다. 이 전에 인간의 신경망에 대한 설명이 나오는데, 차라리 역치 개념을 써놨더라면 ‘아. 신경망과 인공 지능망은 이런 공통점이 있구나!’ 라고 이해했을 걸…</p>

<blockquote>
  <p>하나의 레이어는 여러 개의 노드로 이루어져 있다. 레이어에 속한 하나의 노드에서는 다음과 같은 행동이 발생한다.  (이전 노드의 결과값) * (노드와 노드 간의 가중치)의 결과값 들을 받아들여 모두 더한다.(Wx, linear) 그리고 이 값에 activation함수를 적용해서 다음 노드로 전할 값을 결정한다.(non-linear)</p>
</blockquote>

<p>해당 구조에서 Layer를 여러개 두면 어떻게 될까? 다음과 같은 결과가 나오겠지.</p>

<p><code class="language-plaintext highlighter-rouge">(output) = Wn * max(Wn-1 * max(Wn-2...W2 * max(W1 * x, 0),0),0) (여기서 output layer는 마지막 layer의 노드수 * 1의 크기를 가지는 Wn을 통해 도출된다.)</code></p>

<p>이렇게 되면 해당 layer에 있는 모든 노드들은 다음 노드에 관여하게 되는 구조가 나온다. 그래서 Fully Connected Layer라고 부른다.</p>

<p><img src="https://t1.daumcdn.net/cfile/tistory/99DC70435B57410D02" alt="img" /></p>

<p>이제 다시 돌아가서, 5강의 처음은 Fully Connected Layer(<strong>Dense layer</strong>)에 대한 아이디어로부터 출발하게 된다.</p>

<h4 id="convolutional-neural-networks의-시작">Convolutional Neural Networks의 시작</h4>

<p><img src="https://t1.daumcdn.net/cfile/tistory/991CF9415B61925C1A" alt="img" /></p>

<p>행렬 곱은 (3072 x 1) * (10 x 3072) = (1 * 10) 이 나오게 된다. 이전 시간에 배웠던 Fully Connected Network의 원리를 생각해 보면 모든 input값은 다음 노드로 전달되니까 3072 x 1의 형태로 만든 것이고, hidden layer는 하나, class는 10개라서 output layer 모습이 1 * 10이 된다.</p>

<p>그렇다면 Convolutional Neural Network는 Fully Connected Layer에서 어떻게 나아갔을까?</p>

<p>데이터의 특성을 잘 반영하기 위해서 <strong>filter</strong>의 개념을 도입했다.</p>

<p>경험이 쌓이면 도움이 된다더니, 예전에 막내 국어 문제 풀어줄 때 나왔던 지문이 도움이 될 줄이야.</p>

<p><img src="https://t1.daumcdn.net/cfile/tistory/9989CC495D342D2D2A" alt="img" /></p>

<p>해당 문제는 2018년도 고2 모의평가 3월에 나왔던 지문인데, 풀어주면서도 ‘이걸 애들이 시간내에 풀 수 있을 거라고?’ 하며 연신 고개를 주억거렸던 기억이 난다. 이 그림은 필터를 통해 어떻게 이미지의 특성을 잡아내는지 그 예시를 들었다. 이 다음에 이 이미지를 보고 알맞게 추론한 것을 고르시오. 라는 문제와 함께 그림이 함께 나오는 예시가 나와서 명확히 기억한다.</p>

<p>필터는 이미지의 좌상단부터 훑는다. 그리고 필터의 모든 요소를 가지고 내적을 하며, 하나의 값을 얻게 된다. 이 과정이 위 그림의 첫번째 줄에 해당한다. 이 연산의 결과를 activation map에 저장하게 된다.</p>

<p>32x32x3을 5x5x3의 필터와 활성화 함수로 훑었을 때, 28x28x3의 activation map이 나온다.(n칸씩 뗄 지는 파라미터로 결정 =&gt; stride라고 한다.)</p>

<p><img src="https://t1.daumcdn.net/cfile/tistory/990AEF335B61926033" alt="img" /></p>

<p>정리하면 하나의 CNN은 filtering과 activation을 거친다. 그리고 하나의 activation map은 input data의 한 특성을 나타낸다. 바로 이렇게. 이 과정들을 반복하면  low-level features -&gt; mid-level features -&gt; high-level features 순으로 추출할 수 있다.</p>

<p><img src="https://t1.daumcdn.net/cfile/tistory/9987664E5B61D3FD07" alt="img" /></p>

<p>그래서 전체적인 Convolutional Neural Network는 다음과 같이 생겼다.</p>

<p><img src="https://t1.daumcdn.net/cfile/tistory/99546A3F5B61926206" alt="img" /></p>

<p>그런데 아까전의 예시에서는 한칸씩 필터를 밀었다. 필터를 한칸씩 말고 n칸씩 밀 수 있지 않을까? 이 때 어정쩡하게 남는 부분은 잘리게 되지 않을까? 그리고 input 데이터의 가장자리는 중앙보다 반영되는 횟수가 적지 않을까? 또 n칸씩 확 확 밀면 Layer 몇 개만 거쳐도 중간 값의 차원이 확 줄지 않을까?</p>

<p>그래서 <strong>Padding</strong>의 개념이 나오게 된다. Padding은 원본 데이터의 가장자리에 0으로 채운 행과 열을 더함으로써 위의 두가지 문제를 해결하게 된다.</p>

<p>테두리에 padding을 준다 : (N-F)/(stride+1). padding은 0으로 채운다.</p>

<p>그리고 중간에 이해 안가는 부분이 있어서 데려와봤다.</p>

<p><img src="https://t1.daumcdn.net/cfile/tistory/99CD73375B61D6EF18" alt="img" /></p>

<p>필터를 언급할 때 input 데이터의 depth에 대해서는 생략하고 얘기하더라. 적어도 해당 강의에서는. 그래서 조원들이랑 공부할 때 더더욱 헷갈렸다. 그림을 해석하면 다음과 같다.</p>

<ul>
  <li>1x1 conv with 32 filters : 1x1x64의 filter를 사용하며, 이를 32번 반복한다.</li>
  <li>결과값 56 x56 x 32 : 1x1x64를 한칸씩 밀어서 필터를 적용하고, 이에 activation function을 적용해서 만든 activation map의 크기는 56x56x1인데 이 과정을 32번 반복했으니까 depth는 32.</li>
</ul>

<p><img src="https://t1.daumcdn.net/cfile/tistory/9957CE405B61D6F10C" alt="img" /></p>

<p>해당 그림을 보니까 이전의 그림이 더 명확하게 이해되는데, 5개의 다른 필터를 사용해서 이 결과를 쌓으면 depth가 늘어난다. 그래. 필터를 다른 것을 쓴다고 얘기를 했어야지. 이 때 <code class="language-plaintext highlighter-rouge">필터 = 신경</code> 이렇게 치환해서 생각해보자.</p>

<h4 id="pooling">Pooling</h4>

<p>하지만 이 사이즈를 유지한 채로 계속해서 연산하게 된다면, 이는 성능에 영향을 주게 된다. 그래서 Pooling을 통해 Downsampling을 하게 된다.</p>

<p><img src="https://t1.daumcdn.net/cfile/tistory/99C286405B61D6F325" alt="img" /></p>

<p>해당 그림은 Pooling기법 중 Max를 이용하는 Max Pooling에 대해서 설명하고 있다. 간단하다. 해당 영역에서 max값만 뽑아서 다음 값으로 넘긴다. 마치 그림의 해상도를 n분의 1로 줄일 때 처럼 말이다.</p>

<h4 id="fully-connected-layer-in-cnn">Fully Connected Layer in CNN</h4>

<p>해당 레이어는 마지막에 쓰인다. class를 뽑아내야 하기 때문이다. 우리가 앞에서 Fully Connected Layer는 이런 것이라고 정의했기 때문에, 작동 원리를 생각해서 마지막에 이를 적용하게 되면 우리는 1x(클래스의 수) 크기를 가진 벡터를 얻을 수 있음을 알 수 있다.</p>

<h4 id="오늘의-후기">오늘의 후기</h4>

<p>아…진짜…어후…일단 Fully Connected Layer라는 용어를 유심히 보지 못하고 지나간 것이 화근이었다. 내가 기존에 알고 있던 기초적인 신경망 구조를 Fully Connected Layer라고 부르는 것을 몰랐다.</p>

<p>또 구체적인 예시를 들어야 할 때에는 정작 뭉뚱그려서 설명하는 강의의 방식, 표기법을 통일하지 않는 교재, 심지어 누락까지 되어 있어 이해를 더더욱 어렵게 만든다.</p>

<p>이제 확실히 알겠으니까 다음 강의를 기다려 본다. 이상 저번 강의부터 헷갈린 사람의 후기.</p>
:ET