I". <h1 id="fundamental">Fundamental</h1>

<h2 id="딥러닝-레이어의-이해1--linear-convolution">딥러닝 레이어의 이해(1) : linear, convolution</h2>

<ul>
  <li>레이어의 정의 : 하나의 물체가 여러 개의 논리적인 객체들로 구성되어 있는 경우, 각 개체를 하나의 레이어라 정의함</li>
</ul>

<h3 id="linear-layer">Linear layer</h3>

<ul>
  <li>별칭 : Fully Connected Layer, Feedforward Neural Network, Multilayer perceptrons, Dense Layer…</li>
  <li>Weight의 모든 요소를 Parameter라고 부른다.</li>
</ul>

<p>텐서플로우의 주요 코드 설명</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="c1">### 중간 생략
### (4,2) 차원인 boxes를 (4,3)으로 확장시키는 Linear Layer
</span><span class="n">boxes</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">first_linear</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">first_weight_shape</span> <span class="o">=</span>  <span class="n">first_linear</span><span class="p">.</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">shape</span>

<span class="c1">### 4차원의 입력값을 하나의 실수로 만드는 Linear Layer
### 2단계를 거쳐야 한다
### 먼저 (4,3) -&gt; (4,1) 로 만들어 준다. 
</span><span class="n">second_linear</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">second_out</span> <span class="o">=</span> <span class="n">second_linear</span><span class="p">(</span><span class="n">first_out</span><span class="p">)</span>
<span class="n">second_out</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">second_out</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="c1">### 그리고 (4,1) 을 1차원 데이터로 바꿔주기 위해 Dense를 한번 더 거친다. 
</span><span class="n">third_linear</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">third_out</span> <span class="o">=</span> <span class="n">third_linear</span><span class="p">(</span><span class="n">second_out</span><span class="p">)</span>
<span class="n">third_out</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">third_out</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

</code></pre></div></div>

<ul>
  <li>
    <p>Convolution Layer</p>
  </li>
  <li>
    <p>pooling layer : 핵심만 추리는 것</p>

    <p>Receptive Field : 뉴럴 네트워크의 출력부에서 충분한 정보를 얻기 위해 커버하는 입력 데이터의 인식 영역이 커서, 그 안에 찾고자 하는 객체의 특성이 충분히 포함되어 있어야 정확한 인식이 가능하게 된다.</p>
  </li>
  <li>
    <p>Max pooling의 장점</p>
    <ul>
      <li>translational invariance : 객체 위치에 대한 오버피팅 방지</li>
      <li>non-linear 함수와 동일한 피처 추출 효과</li>
      <li>receptive field 극대화 효과 : max pooling 없이 이런 효과를 보려면 convolutional layer를 수없이 쌓아야 한다.</li>
      <li>cf ) dilated convolution(팽창된 컨볼루션) : 이전의 필터는 가중치들이 모여있는 형태라면 이 필터를 사방으로 밀어서 팽창시킨다. 풍선에 점 찍어놓고 바람 부는 것을 생각해 보자.</li>
    </ul>
  </li>
</ul>

<h2 id="cs231n-10">CS231n-10</h2>

<p>Recurrent Neural Networks(RNN)</p>

<p>vanila neural network : feed-forward neural network(linear layer)</p>

<p>연속적인 데이터에 대해 일어나는 학습의 경우</p>

<p>RNN은 가변길이의 데이터에 대응할 수 있다. 혹은 입/출력이 고정된 경우라 해도 ‘가변 과정’인 경우에 유리하다.</p>

<p>숫자 데이터에 대해서 어떻게 대응하는지 예시를 보여준다.</p>

<p>RNN의 구조는 재귀적이다. 이전 스텝의 결과와 어떤 시점의 벡터를 함수의 입력으로 받는다. 여기서 쓰이는 함수와 함수에서 사용하는 가중치(W)는 변화하지 않는다.</p>

<p>각 단계에서 같은 W를 쓴다.</p>

<p>Many -to-Many의 경우는 각 단계의 출력이 output이고, local loss를 구해서 전체의 loss를 구한다. -&gt; 각 단계에서 개별로 local Gradient들을 구해서 W에 업데이트한다.</p>

<p>Many-to-One의 경우는 마지막 히든 스테이지의 출력이 결과값이 된다.</p>

<p>One-to-Many는 입력은 고정이지만 출력은 가변이다. 입력값이 초기 히든 스테이트를 초기화한다.</p>

<p>그렇다면 Many-to-one과 one-to-many를 합치면? Sequence to Sequence</p>

<p>Encoder(many to one)와 Decoder(one to many)로 이루어진다. 입력을 하나의 백터로 요약하고, 하나의 벡터 입력에서 출력을 얻어낸다. ‘자연어’처리와 관련되어있는 모델이다.</p>

<p>input -&gt; hidden -&gt; output -&gt; softmax(output에서 sample을 뽑기 위해 확률분포를 사용) -&gt; sample -&gt; 다음 인풋으로 sample 넣어주기(input과 sample은 둘 다 정보를 가진 vector. 예를 들면 one-hot-vector로)</p>

<p>근데 왜 확률분포를 사용하는가? 모델에서 다양성을 얻기 위해서. 항상 같은 접두사나 같은 이미지를 넣을 경우 argmax로 sample을 고르면 다양한 값이 나오지 않을 수 있다.</p>

<p>‘sampling’ -&gt; softmax로 확률분포를 취한다고 해도 sampling 설명이 어색해서, ‘teacher forcing’을 도입해서 사용한다. 원래는 확률이 가장 높은 것을 취한다고 해도, ‘의도하는 바(자연어는 말하는 순서가 정해져 있으니까 순서라고 얘기할 수 있다.)를 강요’하는 것이다.</p>

<p>Truncated Backpropagation through time</p>

<p>만약 시퀸스의 길이가 엄청나게 길어진다면 앞부분의 정보가 사라지는 단점이 있다. 그래서 긴 시퀸스를 쪼개서 덩어리로 보는 개념이 Truncated Backpropagation이다. 결국 배치로 잘라서 보는 것이다.</p>

<p>RNN에는 hidden vector가 있고 이 vector를 계속 업데이트한다. RNN의 Cell은 보통 해석하기 힘들다. 해석할 수 있는 cell을 찾을 때 까지 가 보면, 그 모델이 학습하고자 하는 목표에 가까워지는 것을 볼 수 있다.</p>

:ET